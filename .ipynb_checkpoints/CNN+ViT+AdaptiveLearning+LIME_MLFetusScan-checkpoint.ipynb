{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OIHCrxcsQGnu"
   },
   "source": [
    "This pipeline combines DenseNet-121 as the CNN backbone, Swin Transformer-Tiny as the Vision Transformer branch, CBAM for the multi-scale attention module, and LIME for post-hoc explainability, creating a novel and interpretable framework for fetal brain abnormality detection from ultrasound images.\n",
    "\n",
    "**Pipeline Summary**\n",
    "\n",
    "**Data Acquisition and Preprocessing**\n",
    "\n",
    "Uses a fetal brain ultrasound dataset (1,768 images, e.g., Roboflow) across normal and abnormal classes. Preprocessing includes resizing (for CNN/ViT compatibility), normalization, augmentations (to address small data size), and appropriate train-validation-test splits.\n",
    "\n",
    "**CNN Backbone: DenseNet-121**\n",
    "\n",
    "DenseNet-121 extracts local spatial features such as subtle textural changes, anatomical boundaries, and brain structural outlines from ultrasound scans, enhancing detection of localized abnormalities.\n",
    "\n",
    "**Vision Transformer Branch: Swin Transformer-Tiny**\n",
    "\n",
    "Swin Transformer-Tiny provides global contextual understanding, capturing long-range dependencies (such as overall skull shape, large lesions, or abnormal cavity enlargement) that complement the CNN’s localized focus.\n",
    "\n",
    "**Multi-Scale Attention with CBAM**\n",
    "\n",
    "The Convolutional Block Attention Module (CBAM) refines features at both channel and spatial levels, emphasizing image regions and feature groups critical for distinguishing similar abnormalities. This ensures robust attention to both coarse and fine anatomical structures across multiple resolutions.\n",
    "\n",
    "**Feature Fusion**\n",
    "\n",
    "Features from CNN (DenseNet-121) and ViT (Swin Transformer-Tiny) branches are fused (often via concatenation and another multi-scale attention layer) and passed through dense layers to produce unified representations for classification.\n",
    "\n",
    "**Classification**\n",
    "\n",
    "Unified features are processed via fully connected, dropout, and softmax layers for multi-class prediction: normal or subtypes such as ventriculomegaly, encephalocele, holoprosencephaly, and hemorrhage.\n",
    "\n",
    "**Explainability with LIME**\n",
    "\n",
    "LIME (Local Interpretable Model-agnostic Explanations) is used post-classification to generate pixel-level, visually interpretable heatmaps, pinpointing which image areas influenced each decision. This satisfies the need for model transparency, regulatory compliance, and clinician trust—key requirements in medical AI applications.\n",
    "\n",
    "**Key Novelty:**\n",
    "\n",
    "Hybrid CNN-ViT architecture enables robust detection across local and global feature scales.\n",
    "\n",
    "Multi-scale attention (CBAM) ensures adaptive focus on medically relevant regions.\n",
    "\n",
    "LIME provides granular, model-agnostic explainability, addressing the research gap in transparent, clinically deployable fetal abnormality detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wgPtCL68P6uS",
    "outputId": "b20a7e65-1b34-44f6-dab2-6e1b1947be9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package            Version\n",
      "------------------ ------------\n",
      "certifi            2025.8.3\n",
      "charset-normalizer 3.4.3\n",
      "colorama           0.4.6\n",
      "contourpy          1.3.3\n",
      "cycler             0.12.1\n",
      "filelock           3.19.1\n",
      "fonttools          4.60.1\n",
      "fsspec             2025.9.0\n",
      "huggingface-hub    0.35.3\n",
      "idna               3.10\n",
      "imageio            2.37.0\n",
      "Jinja2             3.1.6\n",
      "joblib             1.5.2\n",
      "kiwisolver         1.4.9\n",
      "lazy_loader        0.4\n",
      "lime               0.2.0.1\n",
      "MarkupSafe         3.0.3\n",
      "matplotlib         3.10.6\n",
      "mpmath             1.3.0\n",
      "networkx           3.5\n",
      "numpy              2.2.6\n",
      "opencv-python      4.12.0.88\n",
      "packaging          25.0\n",
      "pandas             2.3.3\n",
      "pillow             11.3.0\n",
      "pip                24.0\n",
      "pyparsing          3.2.5\n",
      "python-dateutil    2.9.0.post0\n",
      "pytz               2025.2\n",
      "PyYAML             6.0.3\n",
      "requests           2.32.5\n",
      "safetensors        0.6.2\n",
      "scikit-image       0.25.2\n",
      "scikit-learn       1.7.2\n",
      "scipy              1.16.2\n",
      "seaborn            0.13.2\n",
      "setuptools         80.9.0\n",
      "six                1.17.0\n",
      "sympy              1.14.0\n",
      "threadpoolctl      3.6.0\n",
      "tifffile           2025.9.30\n",
      "timm               1.0.20\n",
      "torch              2.2.2+cu121\n",
      "torchaudio         2.2.2+cu121\n",
      "torchvision        0.17.2+cu121\n",
      "tqdm               4.67.1\n",
      "typing_extensions  4.15.0\n",
      "tzdata             2025.2\n",
      "urllib3            2.5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bHPAnEJbX_-E",
    "outputId": "66346151-0b7f-48f6-bff8-336113c0ab35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\OneDrive\\Desktop\\ML_Endsem\\venv\\Scripts\\python.exe\n",
      "['C:\\\\Users\\\\andre\\\\anaconda3\\\\python312.zip', 'C:\\\\Users\\\\andre\\\\anaconda3\\\\DLLs', 'C:\\\\Users\\\\andre\\\\anaconda3\\\\Lib', 'C:\\\\Users\\\\andre\\\\anaconda3', 'C:\\\\Users\\\\andre\\\\OneDrive\\\\Desktop\\\\ML_Endsem\\\\venv']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "print(sys.path[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 550
    },
    "id": "R5J74kQMXjgI",
    "outputId": "f3d29576-d02e-4feb-abb6-a9b2f42f6243"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: [' anold-chiari-malformation', ' arachnoid-cyst', ' cerebellah-hypoplasia', ' colphocephaly', ' encephalocele', ' holoprosencephaly', ' hydracenphaly', ' intracranial-hemorrdge', ' intracranial-tumor', ' m-magna', ' mild-ventriculomegaly', ' moderate-ventriculomegaly', ' normal', ' polencephaly', ' severe-ventriculomegaly', ' vein-of-galen']\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'T' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 348\u001b[39m\n\u001b[32m    345\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m348\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 267\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    264\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mClasses:\u001b[39m\u001b[33m\"\u001b[39m, class_names)\n\u001b[32m    266\u001b[39m \u001b[38;5;66;03m# transforms\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m267\u001b[39m train_transform = \u001b[43mT\u001b[49m.Compose([\n\u001b[32m    268\u001b[39m     T.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n\u001b[32m    269\u001b[39m     T.RandomHorizontalFlip(),\n\u001b[32m    270\u001b[39m     T.RandomRotation(\u001b[32m10\u001b[39m),\n\u001b[32m    271\u001b[39m     T.ColorJitter(\u001b[32m0.1\u001b[39m,\u001b[32m0.1\u001b[39m),\n\u001b[32m    272\u001b[39m     T.Grayscale(\u001b[32m3\u001b[39m),\n\u001b[32m    273\u001b[39m     T.ToTensor(),\n\u001b[32m    274\u001b[39m     T.Normalize([\u001b[32m0.485\u001b[39m,\u001b[32m0.456\u001b[39m,\u001b[32m0.406\u001b[39m],[\u001b[32m0.229\u001b[39m,\u001b[32m0.224\u001b[39m,\u001b[32m0.225\u001b[39m])\n\u001b[32m    275\u001b[39m ])\n\u001b[32m    276\u001b[39m val_transform = T.Compose([\n\u001b[32m    277\u001b[39m     T.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n\u001b[32m    278\u001b[39m     T.Grayscale(\u001b[32m3\u001b[39m),\n\u001b[32m    279\u001b[39m     T.ToTensor(),\n\u001b[32m    280\u001b[39m     T.Normalize([\u001b[32m0.485\u001b[39m,\u001b[32m0.456\u001b[39m,\u001b[32m0.406\u001b[39m],[\u001b[32m0.229\u001b[39m,\u001b[32m0.224\u001b[39m,\u001b[32m0.225\u001b[39m])\n\u001b[32m    281\u001b[39m ])\n\u001b[32m    283\u001b[39m train_ds = FetalUSDataset(train_dir, IMAGE_SIZE, train_transform)\n",
      "\u001b[31mNameError\u001b[39m: name 'T' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "fetal_brain_pipeline_multiclass.py\n",
    "\n",
    "End-to-end pipeline (multi-class):\n",
    "- dataset loader (reads <split>/_classes.csv with one-hot labels)\n",
    "- model: DenseNet-121 (CNN) + Swin-Tiny (ViT) + CBAM (attention) + fusion head\n",
    "- training loop (CrossEntropyLoss)\n",
    "- validation/test metrics (per-class AUC, F1 macro/micro)\n",
    "- LIME explanation generation (save heatmaps)\n",
    "\n",
    "Author: ChatGPT\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "import timm\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_recall_fscore_support\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# LIME imports\n",
    "from lime import lime_image\n",
    "from skimage.segmentation import mark_boundaries\n",
    "\n",
    "# -------------------------\n",
    "# Config / hyperparameters\n",
    "# -------------------------\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "DATA_ROOT = \"Classification_Dataset\"\n",
    "BATCH_SIZE = 16\n",
    "IMAGE_SIZE = 224\n",
    "NUM_EPOCHS = 10\n",
    "LR = 1e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "NUM_WORKERS = 6\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MODEL_SAVE_PATH = \"best_model_multiclass.pth\"\n",
    "NUM_CLASSES = 11  # update if different; matches your CSV header count\n",
    "\n",
    "# -------------------------\n",
    "# Helper: read CSV, get classes\n",
    "# -------------------------\n",
    "def read_classes_from_csv(csv_path: str):\n",
    "    df = pd.read_csv(csv_path, sep=None, engine=\"python\")\n",
    "    class_cols = list(df.columns[1:])\n",
    "    return class_cols\n",
    "\n",
    "# -------------------------\n",
    "# Dataset\n",
    "# -------------------------\n",
    "class FetalUSDataset(Dataset):\n",
    "    def __init__(self, split_dir: str, image_size=224, transform=None):\n",
    "        csv_path = os.path.join(split_dir, \"_classes.csv\")\n",
    "        assert os.path.exists(csv_path), f\"CSV not found: {csv_path}\"\n",
    "        self.df = pd.read_csv(csv_path, sep=None, engine=\"python\")\n",
    "        self.dir = split_dir\n",
    "        self.filenames = self.df.iloc[:, 0].astype(str).values\n",
    "        self.labels = self.df.iloc[:, 1:].astype(int).values\n",
    "        self.transform = transform if transform else self.default_transform(image_size)\n",
    "        self.image_size = image_size\n",
    "\n",
    "    def default_transform(self, image_size):\n",
    "        return T.Compose([\n",
    "            T.Resize((image_size, image_size)),\n",
    "            T.Grayscale(num_output_channels=3),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fn = self.filenames[idx]\n",
    "        img_path = os.path.join(self.dir, fn)\n",
    "        if not os.path.exists(img_path):\n",
    "            alt = os.path.join(self.dir, \"images\", fn)\n",
    "            if os.path.exists(alt):\n",
    "                img_path = alt\n",
    "            else:\n",
    "                raise FileNotFoundError(f\"{img_path} missing\")\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        x = self.transform(img)\n",
    "        y = torch.tensor(np.argmax(self.labels[idx]), dtype=torch.long)  # convert one-hot → class index\n",
    "        return x, y, fn\n",
    "\n",
    "# -------------------------\n",
    "# CBAM implementation\n",
    "# -------------------------\n",
    "class BasicConv(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, bias=False):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size, stride=stride, padding=padding, bias=bias)\n",
    "        self.bn = nn.BatchNorm2d(out_planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    def forward(self, x):\n",
    "        return self.relu(self.bn(self.conv(x)))\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_planes, ratio=16):\n",
    "        super().__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        self.fc1 = nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))\n",
    "        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))\n",
    "        return self.sigmoid(avg_out + max_out)\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super().__init__()\n",
    "        padding = 3 if kernel_size==7 else 1\n",
    "        self.conv = BasicConv(2, 1, kernel_size, padding=padding, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x_cat = torch.cat([avg_out, max_out], dim=1)\n",
    "        return self.sigmoid(self.conv(x_cat))\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, in_planes, ratio=16, kernel_size=7):\n",
    "        super().__init__()\n",
    "        self.channel_att = ChannelAttention(in_planes, ratio)\n",
    "        self.spatial_att = SpatialAttention(kernel_size)\n",
    "    def forward(self, x):\n",
    "        x_out = x * self.channel_att(x)\n",
    "        return x_out * self.spatial_att(x_out)\n",
    "\n",
    "# -------------------------\n",
    "# HybridNet: DenseNet + CBAM + Swin + Fusion\n",
    "# -------------------------\n",
    "class HybridNet(nn.Module):\n",
    "    def __init__(self, densenet_model_name=\"densenet121\", swin_model_name=\"swin_base_patch4_window7_224\", num_classes=11):\n",
    "        super().__init__()\n",
    "\n",
    "        # CNN branch\n",
    "        dnet = timm.create_model(densenet_model_name, pretrained=True)\n",
    "        self.cnn_features = dnet.features\n",
    "        self.cbam = CBAM(dnet.num_features)\n",
    "        self.cnn_gap = nn.AdaptiveAvgPool2d((1,1))\n",
    "        cnn_feat_dim = dnet.num_features\n",
    "\n",
    "        # Swin branch\n",
    "        self.swin = timm.create_model(swin_model_name, pretrained=True, num_classes=0, global_pool=\"avg\")\n",
    "        swin_feat_dim = self.swin.num_features\n",
    "        self.swin_pool = nn.AdaptiveAvgPool2d((1,1))\n",
    "\n",
    "        # Fusion + classifier\n",
    "        fusion_in_features = cnn_feat_dim + swin_feat_dim  # automatically 1024 + 1007\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(fusion_in_features, 512),  # use correct input dim\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # CNN\n",
    "        cnn_feat = self.cnn_features(x)\n",
    "        cnn_feat = self.cbam(cnn_feat)\n",
    "        cnn_vec = torch.flatten(self.cnn_gap(cnn_feat), 1)\n",
    "        # Swin\n",
    "        swin_feat = self.swin.forward_features(x)\n",
    "        swin_vec = torch.flatten(self.swin_pool(swin_feat), 1)\n",
    "        # Fusion\n",
    "        fused = torch.cat([cnn_vec, swin_vec], dim=1)\n",
    "        fused = self.fusion_dropout(fused)\n",
    "        logits = self.classifier(fused)\n",
    "        return logits\n",
    "\n",
    "# -------------------------\n",
    "# Training / validation\n",
    "# -------------------------\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for imgs, labels, _ in tqdm(loader, desc=\"Train batches\"):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(imgs)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "    return running_loss / len(loader.dataset)\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, loader, device):\n",
    "    model.eval()\n",
    "    all_logits, all_labels, filenames = [], [], []\n",
    "    for imgs, labels, fns in tqdm(loader, desc=\"Validation batches\"):\n",
    "        imgs = imgs.to(device)\n",
    "        logits = model(imgs)\n",
    "        all_logits.append(logits.detach().cpu().numpy())\n",
    "        all_labels.append(labels.numpy())\n",
    "        filenames.extend(fns)\n",
    "    all_logits = np.vstack(all_logits)\n",
    "    all_labels = np.hstack(all_labels)\n",
    "    return all_labels, all_logits, filenames\n",
    "\n",
    "# -------------------------\n",
    "# LIME explainer\n",
    "# -------------------------\n",
    "class LimeExplainerWrapper:\n",
    "    def __init__(self, model, class_names, device=\"cpu\", transform=None):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        self.class_names = class_names\n",
    "        self.device = device\n",
    "        self.transform = transform if transform else T.Compose([\n",
    "            T.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "        ])\n",
    "    def predict_proba(self, imgs_rgb):\n",
    "        batch = []\n",
    "        for im in imgs_rgb:\n",
    "            pil = Image.fromarray(im.astype(np.uint8))\n",
    "            x = self.transform(pil)\n",
    "            batch.append(x)\n",
    "        x = torch.stack(batch).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(x)\n",
    "            probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
    "        return probs\n",
    "    def explain_image(self, img_path, label_idx, top_labels=1, num_samples=1000, hide_color=0):\n",
    "        explainer = lime_image.LimeImageExplainer()\n",
    "        image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "        fn = lambda imgs: self.predict_proba(np.array(imgs))\n",
    "        explanation = explainer.explain_instance(image, fn, top_labels=top_labels, hide_color=hide_color, num_samples=num_samples)\n",
    "        temp, mask = explanation.get_image_and_mask(label_idx, positive_only=True, num_features=5, hide_rest=False)\n",
    "        vis = mark_boundaries(temp/255.0, mask)\n",
    "        return vis, explanation\n",
    "\n",
    "# -------------------------\n",
    "# Main\n",
    "# -------------------------\n",
    "def main():\n",
    "    # splits\n",
    "    train_dir, valid_dir, test_dir = [os.path.join(DATA_ROOT, s) for s in [\"train\",\"valid\",\"test\"]]\n",
    "    class_names = read_classes_from_csv(os.path.join(train_dir,\"_classes.csv\"))\n",
    "    global NUM_CLASSES\n",
    "    NUM_CLASSES = len(class_names)\n",
    "    print(\"Classes:\", class_names)\n",
    "\n",
    "    # transforms\n",
    "    train_transform = T.Compose([\n",
    "        T.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "        T.RandomHorizontalFlip(),\n",
    "        T.RandomRotation(10),\n",
    "        T.ColorJitter(0.1,0.1),\n",
    "        T.Grayscale(3),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "    ])\n",
    "    val_transform = T.Compose([\n",
    "        T.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "        T.Grayscale(3),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "    ])\n",
    "\n",
    "    train_ds = FetalUSDataset(train_dir, IMAGE_SIZE, train_transform)\n",
    "    val_ds = FetalUSDataset(valid_dir, IMAGE_SIZE, val_transform)\n",
    "    test_ds = FetalUSDataset(test_dir, IMAGE_SIZE, val_transform)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "    # model, criterion, optimizer\n",
    "    model = HybridNet(num_classes=NUM_CLASSES).to(DEVICE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)\n",
    "\n",
    "    best_val_f1 = -1.0\n",
    "    for epoch in range(1, NUM_EPOCHS+1):\n",
    "        print(f\"\\nEpoch {epoch}/{NUM_EPOCHS}\")\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, DEVICE)\n",
    "        print(f\"Train loss: {train_loss:.4f}\")\n",
    "\n",
    "        val_labels, val_logits, _ = validate(model, val_loader, DEVICE)\n",
    "        val_preds = np.argmax(val_logits, axis=1)\n",
    "        f1_macro = f1_score(val_labels, val_preds, average=\"macro\")\n",
    "        f1_micro = f1_score(val_labels, val_preds, average=\"micro\")\n",
    "        print(f\"Val F1 macro: {f1_macro:.4f} | F1 micro: {f1_micro:.4f}\")\n",
    "\n",
    "        if f1_macro > best_val_f1:\n",
    "            best_val_f1 = f1_macro\n",
    "            torch.save({\"model_state\": model.state_dict(), \"class_names\": class_names, \"epoch\": epoch}, MODEL_SAVE_PATH)\n",
    "            print(\"Saved best model.\")\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    # Test evaluation\n",
    "    ckpt = torch.load(MODEL_SAVE_PATH, map_location=DEVICE)\n",
    "    model.load_state_dict(ckpt[\"model_state\"])\n",
    "    test_labels, test_logits, test_fns = validate(model, test_loader, DEVICE)\n",
    "    test_preds = np.argmax(test_logits, axis=1)\n",
    "    f1_macro = f1_score(test_labels, test_preds, average=\"macro\")\n",
    "    f1_micro = f1_score(test_labels, test_preds, average=\"micro\")\n",
    "    print(\"Test F1 macro:\", f1_macro, \"F1 micro:\", f1_micro)\n",
    "\n",
    "    # Save test predictions\n",
    "    test_probs = torch.softmax(torch.tensor(test_logits), dim=1).numpy()\n",
    "    out_df = pd.DataFrame(test_probs, columns=class_names)\n",
    "    out_df[\"filename\"] = test_fns\n",
    "    out_df = out_df[[\"filename\"] + class_names]\n",
    "    out_df.to_csv(\"test_predictions_multiclass.csv\", index=False)\n",
    "    print(\"Saved test_predictions_multiclass.csv\")\n",
    "\n",
    "    # Example LIME explanation\n",
    "    explainer = LimeExplainerWrapper(model, class_names, DEVICE, val_transform)\n",
    "    target_class_name = class_names[0]  # e.g., first class\n",
    "    idx = class_names.index(target_class_name)\n",
    "    for i in range(len(test_ds)):\n",
    "        _, label_idx, fn = test_ds[i]\n",
    "        if label_idx == idx:\n",
    "            img_path = os.path.join(test_dir, fn)\n",
    "            vis, _ = explainer.explain_image(img_path, label_idx=idx, num_samples=800)\n",
    "            out_img = f\"lime_{os.path.basename(fn)}_{target_class_name}.png\"\n",
    "            plt.imsave(out_img, vis)\n",
    "            print(\"Saved LIME visualization:\", out_img)\n",
    "            break\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.git', '.gitignore', '.ipynb_checkpoints', 'Classification_Dataset', 'CNN+ViT+AdaptiveLearning+LIME_MLFetusScan.ipynb', 'README.md', 'requirements.txt', 'setup.py', 'venv']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir('.'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (ml_endsem)",
   "language": "python",
   "name": "ml_endsem"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
