{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8e29d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "fetal_brain_pipeline_multiclass_with_crossattn_gradcam.py\n",
    "\n",
    "Full pipeline (multi-class) â€” updated to include:\n",
    "- Cross-Attention Fusion (CNN <-> ViT)\n",
    "- Grad-CAM explanations (replaces LIME example at end)\n",
    "\n",
    "Other pipeline parts are unchanged: Dataset, CBAM, DenseNet + Swin, training loop, metrics, saving.\n",
    "\n",
    "Author: ChatGPT (integrated into your existing script)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "import timm\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_recall_fscore_support\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# -------------------------\n",
    "# Config / hyperparameters\n",
    "# -------------------------\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "DATA_ROOT = \"Classification_Dataset\"\n",
    "BATCH_SIZE = 2\n",
    "IMAGE_SIZE = 224\n",
    "NUM_EPOCHS = 10\n",
    "LR = 1e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "NUM_WORKERS = 0\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MODEL_SAVE_PATH = \"best_model_multiclass.pth\"\n",
    "NUM_CLASSES = 11  # will be overridden after reading CSV if different\n",
    "\n",
    "# -------------------------\n",
    "# Helper: read CSV, get classes\n",
    "# -------------------------\n",
    "def read_classes_from_csv(csv_path: str):\n",
    "    # Use sep=None to auto-detect delimiter, engine='python' for flexibility\n",
    "    df = pd.read_csv(csv_path, sep=None, engine=\"python\")\n",
    "    class_cols = list(df.columns[1:])\n",
    "    return class_cols\n",
    "\n",
    "# -------------------------\n",
    "# Dataset\n",
    "# -------------------------\n",
    "class FetalUSDataset(Dataset):\n",
    "    def __init__(self, split_dir: str, image_size=224, transform=None):\n",
    "        csv_path = os.path.join(split_dir, \"_classes.csv\")\n",
    "        assert os.path.exists(csv_path), f\"CSV not found: {csv_path}\"\n",
    "        self.df = pd.read_csv(csv_path, sep=None, engine=\"python\")\n",
    "        self.dir = split_dir\n",
    "        self.filenames = self.df.iloc[:, 0].astype(str).values\n",
    "        self.labels = self.df.iloc[:, 1:].astype(int).values\n",
    "        self.transform = transform if transform else self.default_transform(image_size)\n",
    "        self.image_size = image_size\n",
    "\n",
    "    def default_transform(self, image_size):\n",
    "        return T.Compose([\n",
    "            T.Resize((image_size, image_size)),\n",
    "            T.Grayscale(num_output_channels=3),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fn = self.filenames[idx]\n",
    "        img_path = os.path.join(self.dir, fn)\n",
    "        if not os.path.exists(img_path):\n",
    "            alt = os.path.join(self.dir, \"images\", fn)\n",
    "            if os.path.exists(alt):\n",
    "                img_path = alt\n",
    "            else:\n",
    "                raise FileNotFoundError(f\"File {fn} not found in {self.dir} or {os.path.join(self.dir, 'images')}\")\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        x = self.transform(img)\n",
    "        y = torch.tensor(np.argmax(self.labels[idx]), dtype=torch.long)  # one-hot -> index\n",
    "        return x, y, fn\n",
    "\n",
    "# -------------------------\n",
    "# CBAM implementation\n",
    "# -------------------------\n",
    "class BasicConv(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, bias=False):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size, stride=stride, padding=padding, bias=bias)\n",
    "        self.bn = nn.BatchNorm2d(out_planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    def forward(self, x):\n",
    "        return self.relu(self.bn(self.conv(x)))\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_planes, ratio=16):\n",
    "        super().__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        self.fc1 = nn.Conv2d(in_planes, max(in_planes // ratio, 1), 1, bias=False)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Conv2d(max(in_planes // ratio, 1), in_planes, 1, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))\n",
    "        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))\n",
    "        return self.sigmoid(avg_out + max_out)\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super().__init__()\n",
    "        padding = 3 if kernel_size==7 else 1\n",
    "        self.conv = BasicConv(2, 1, kernel_size, padding=padding, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x_cat = torch.cat([avg_out, max_out], dim=1)\n",
    "        return self.sigmoid(self.conv(x_cat))\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, in_planes, ratio=16, kernel_size=7):\n",
    "        super().__init__()\n",
    "        self.channel_att = ChannelAttention(in_planes, ratio)\n",
    "        self.spatial_att = SpatialAttention(kernel_size)\n",
    "    def forward(self, x):\n",
    "        x_out = x * self.channel_att(x)\n",
    "        return x_out * self.spatial_att(x_out)\n",
    "\n",
    "# -------------------------\n",
    "# Cross-Attention Fusion module\n",
    "# -------------------------\n",
    "class CrossAttentionFusion(nn.Module):\n",
    "    \"\"\"\n",
    "    Bidirectional cross-attention between CNN spatial tokens and ViT tokens.\n",
    "    - Projects both feature maps to same embed dim\n",
    "    - Flattens spatial dims to token sequences\n",
    "    - Applies MultiheadAttention in both directions\n",
    "    - Pools refined token sequences to vectors for classification\n",
    "    \"\"\"\n",
    "    def __init__(self, cnn_channels, vit_channels, embed_dim=256, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.cnn_proj = nn.Conv2d(cnn_channels, embed_dim, kernel_size=1)\n",
    "        self.vit_proj = nn.Conv2d(vit_channels, embed_dim, kernel_size=1)\n",
    "        self.attn_c2v = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
    "        self.attn_v2c = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
    "        self.norm_c = nn.LayerNorm(embed_dim)\n",
    "        self.norm_v = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, cnn_feat_map, vit_feat_map):\n",
    "        \"\"\"\n",
    "        cnn_feat_map: [B, Cc, Hc, Wc]\n",
    "        vit_feat_map: [B, Cv, Hv, Wv]\n",
    "        returns: cnn_vec [B, E], vit_vec [B, E]\n",
    "        \"\"\"\n",
    "        B = cnn_feat_map.shape[0]\n",
    "        # project\n",
    "        c = self.cnn_proj(cnn_feat_map)   # [B, E, Hc, Wc]\n",
    "        v = self.vit_proj(vit_feat_map)   # [B, E, Hv, Wv]\n",
    "\n",
    "        # flatten spatial dims -> sequences: [B, N, E]\n",
    "        c_seq = c.flatten(2).permute(0, 2, 1).contiguous()  # [B, Hc*Wc, E]\n",
    "        v_seq = v.flatten(2).permute(0, 2, 1).contiguous()  # [B, Hv*Wv, E]\n",
    "\n",
    "        # cross-attention: CNN queries Vit (so keys/values from vit)\n",
    "        # MultiheadAttention batch_first=True accepts (B, N, E)\n",
    "        c_refined, _ = self.attn_c2v(query=c_seq, key=v_seq, value=v_seq)\n",
    "        v_refined, _ = self.attn_v2c(query=v_seq, key=c_seq, value=c_seq)\n",
    "\n",
    "        # LayerNorm\n",
    "        c_refined = self.norm_c(c_refined)\n",
    "        v_refined = self.norm_v(v_refined)\n",
    "\n",
    "        # Pool along token dimension -> vectors\n",
    "        c_vec = c_refined.mean(dim=1)  # [B, E]\n",
    "        v_vec = v_refined.mean(dim=1)  # [B, E]\n",
    "\n",
    "        return c_vec, v_vec\n",
    "\n",
    "# -------------------------\n",
    "# Grad-CAM utility\n",
    "# -------------------------\n",
    "class GradCAM:\n",
    "    \"\"\"\n",
    "    Simple Grad-CAM implementation hooking into the CNN feature map output (cnn_features).\n",
    "    Use generate_cam(input_tensor, target_class) to obtain a heatmap (H_in x W_in).\n",
    "    \"\"\"\n",
    "    def __init__(self, model: nn.Module, target_module_name=\"cnn_features\", device=\"cpu\"):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        self.device = device\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "        \n",
    "        # Find the target module\n",
    "        target_module = None\n",
    "        for name, module in self.model.named_modules():\n",
    "            if name == target_module_name:\n",
    "                target_module = module\n",
    "                break\n",
    "        if target_module is None:\n",
    "             raise ValueError(f\"Target module '{target_module_name}' not found in model.\")\n",
    "             \n",
    "        # forward hook to save activations\n",
    "        def forward_hook(module, input, output):\n",
    "            self.activations = output.detach()\n",
    "        def backward_hook(module, grad_in, grad_out):\n",
    "            # grad_out is a tuple with gradients w.r.t. outputs\n",
    "            self.gradients = grad_out[0].detach()\n",
    "        # register\n",
    "        self.f_hook = target_module.register_forward_hook(forward_hook)\n",
    "        self.b_hook = target_module.register_backward_hook(backward_hook)\n",
    "\n",
    "    def generate_cam(self, input_tensor: torch.Tensor, target_index: int = None):\n",
    "        \"\"\"\n",
    "        input_tensor: single image tensor [1,3,H,W] normalized as model expects\n",
    "        target_index: class index for which to compute Grad-CAM. If None, uses top predicted class.\n",
    "        returns: heatmap numpy array scaled to input HxW (values 0..1)\n",
    "        \"\"\"\n",
    "        self.model.zero_grad()\n",
    "        input_tensor = input_tensor.to(self.device)\n",
    "        logits = self.model(input_tensor)  # [1, C]\n",
    "        if target_index is None:\n",
    "            target_index = torch.argmax(logits, dim=1).item()\n",
    "        score = logits[:, target_index]\n",
    "        score.backward(retain_graph=True)\n",
    "\n",
    "        # get gradients and activations\n",
    "        grads = self.gradients  # [B, Cc, Hc, Wc]\n",
    "        acts = self.activations  # [B, Cc, Hc, Wc]\n",
    "        if grads is None or acts is None:\n",
    "            raise RuntimeError(\"Gradients or activations not captured. Make sure forward() was called on model and hooks are registered.\")\n",
    "\n",
    "        # global-average-pool gradients -> weights [B, Cc]\n",
    "        weights = torch.mean(grads.view(grads.shape[0], grads.shape[1], -1), dim=2)  # [B, Cc]\n",
    "\n",
    "        # weighted combination of activations\n",
    "        # We only operate on the first item in batch (B=1)\n",
    "        cam = torch.zeros(acts.shape[2], acts.shape[3], device=self.device) # Hc, Wc\n",
    "        for i in range(weights.shape[1]): # Cc\n",
    "            cam += weights[0, i] * acts[0, i, :, :]\n",
    "\n",
    "        cam = F.relu(cam)\n",
    "        # normalize cam to [0,1]\n",
    "        cam_min = cam.min()\n",
    "        cam_max = cam.max()\n",
    "        if cam_max > cam_min and cam_max > 0:\n",
    "            cam = (cam - cam_min) / (cam_max - cam_min)\n",
    "        elif cam_max > 0:\n",
    "             cam = cam / cam_max\n",
    "\n",
    "        # upsample to input size\n",
    "        cam = cam.unsqueeze(0).unsqueeze(0)  # [1,1,Hc,Wc]\n",
    "        cam_up = F.interpolate(cam, size=(input_tensor.shape[2], input_tensor.shape[3]), mode='bilinear', align_corners=False)\n",
    "        cam_up = cam_up.squeeze().cpu().numpy()\n",
    "        return cam_up\n",
    "\n",
    "    def remove_hooks(self):\n",
    "        self.f_hook.remove()\n",
    "        self.b_hook.remove()\n",
    "\n",
    "# -------------------------\n",
    "# HybridNet: DenseNet + CBAM + Swin + Cross-Attn Fusion\n",
    "# -------------------------\n",
    "class HybridNet(nn.Module):\n",
    "    def __init__(self, densenet_model_name=\"densenet121\", swin_model_name=\"swin_base_patch4_window7_224\", num_classes=11, image_size=224, device=\"cpu\", cross_embed_dim=256, cross_heads=8):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "        # CNN branch (DenseNet)\n",
    "        dnet = timm.create_model(densenet_model_name, pretrained=True)\n",
    "        self.cnn_features = dnet.features            # will output [B, Cc, Hc, Wc]\n",
    "        cnn_feat_channels = dnet.num_features\n",
    "\n",
    "        # CBAM on CNN feature map\n",
    "        self.cbam = CBAM(cnn_feat_channels)\n",
    "\n",
    "        # global pool for fallback vector (not used for cross-attn fusion)\n",
    "        self.cnn_gap = nn.AdaptiveAvgPool2d((1,1))\n",
    "\n",
    "        # Swin branch\n",
    "        # FIX: global_pool=\"\" to output feature map\n",
    "        self.swin = timm.create_model(swin_model_name, pretrained=True, num_classes=0, global_pool=\"\")\n",
    "        swin_feat_channels = self.swin.num_features\n",
    "        self.swin_pool = nn.AdaptiveAvgPool2d((1,1)) # This is now unused, but harmless\n",
    "\n",
    "        # Cross-attention fusion module\n",
    "        self.cross_fusion = CrossAttentionFusion(cnn_channels=cnn_feat_channels, vit_channels=swin_feat_channels,\n",
    "                                                  embed_dim=cross_embed_dim, num_heads=cross_heads)\n",
    "\n",
    "        # Classifier - input dimension = 2 * cross_embed_dim (cnn_vec + vit_vec)\n",
    "        fusion_in_features = cross_embed_dim * 2\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(fusion_in_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # CNN branch\n",
    "        cnn_feat = self.cnn_features(x)    # [B, Cc, Hc, Wc]\n",
    "        cnn_feat = self.cbam(cnn_feat)     # apply CBAM\n",
    "\n",
    "        # Swin branch\n",
    "        # This outputs [B, H, W, C], e.g. [B, 7, 7, 1024] (channels-last)\n",
    "        swin_feat = self.swin.forward_features(x)\n",
    "\n",
    "        # --- FIX: Permute from channels-last [B, H, W, C] to channels-first [B, C, H, W] ---\n",
    "        # Conv2d layers in cross_fusion expect [B, C, H, W]\n",
    "        swin_feat = swin_feat.permute(0, 3, 1, 2).contiguous() # Shape becomes [B, 1024, 7, 7]\n",
    "        \n",
    "        # Cross-attention fusion -> returns pooled vectors\n",
    "        cnn_vec, vit_vec = self.cross_fusion(cnn_feat, swin_feat)  # each [B, E]\n",
    "\n",
    "        # fuse and classify\n",
    "        fused = torch.cat([cnn_vec, vit_vec], dim=1)  # [B, 2*E]\n",
    "        logits = self.classifier(fused)\n",
    "        return logits\n",
    "\n",
    "# -------------------------\n",
    "# Training / validation (unchanged)\n",
    "# -------------------------\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for imgs, labels, _ in tqdm(loader, desc=\"Train batches\"):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(imgs)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "    return running_loss / len(loader.dataset)\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, loader, device):\n",
    "    model.eval()\n",
    "    all_logits, all_labels, filenames = [], [], []\n",
    "    for imgs, labels, fns in tqdm(loader, desc=\"Validation batches\"):\n",
    "        imgs = imgs.to(device)\n",
    "        logits = model(imgs)\n",
    "        all_logits.append(logits.detach().cpu().numpy())\n",
    "        all_labels.append(labels.numpy())\n",
    "        filenames.extend(fns)\n",
    "    all_logits = np.vstack(all_logits)\n",
    "    all_labels = np.hstack(all_labels)\n",
    "    return all_labels, all_logits, filenames\n",
    "\n",
    "# -------------------------\n",
    "# Main\n",
    "# -------------------------\n",
    "def main():\n",
    "    # splits\n",
    "    train_dir, valid_dir, test_dir = [os.path.join(DATA_ROOT, s) for s in [\"train\",\"valid\",\"test\"]]\n",
    "    \n",
    "    # Ensure train directory and its _classes.csv exist before proceeding\n",
    "    train_csv_path = os.path.join(train_dir, \"_classes.csv\")\n",
    "    if not os.path.exists(train_csv_path):\n",
    "        print(f\"Error: Training CSV not found at {train_csv_path}\")\n",
    "        print(\"Please ensure DATA_ROOT is set correctly and your 'train' folder contains '_classes.csv'\")\n",
    "        return\n",
    "\n",
    "    class_names = read_classes_from_csv(train_csv_path)\n",
    "    global NUM_CLASSES\n",
    "    NUM_CLASSES = len(class_names)\n",
    "    print(\"Classes:\", class_names)\n",
    "    print(f\"Found {NUM_CLASSES} classes.\")\n",
    "\n",
    "    # transforms\n",
    "    train_transform = T.Compose([\n",
    "        T.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "        T.RandomHorizontalFlip(),\n",
    "        T.RandomRotation(10),\n",
    "        T.ColorJitter(0.1,0.1),\n",
    "        T.Grayscale(3),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "    ])\n",
    "    val_transform = T.Compose([\n",
    "        T.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "        T.Grayscale(3),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "    ])\n",
    "\n",
    "    train_ds = FetalUSDataset(train_dir, IMAGE_SIZE, train_transform)\n",
    "    val_ds = FetalUSDataset(valid_dir, IMAGE_SIZE, val_transform)\n",
    "    test_ds = FetalUSDataset(test_dir, IMAGE_SIZE, val_transform)\n",
    "\n",
    "    print(f\"Dataset loaded: {len(train_ds)} train, {len(val_ds)} val, {len(test_ds)} test examples.\")\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "    # model, criterion, optimizer\n",
    "    model = HybridNet(num_classes=NUM_CLASSES, image_size=IMAGE_SIZE, device=DEVICE).to(DEVICE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)\n",
    "\n",
    "    print(f\"Starting training for {NUM_EPOCHS} epochs on {DEVICE}...\")\n",
    "    best_val_f1 = -1.0\n",
    "    for epoch in range(1, NUM_EPOCHS+1):\n",
    "        print(f\"\\nEpoch {epoch}/{NUM_EPOCHS}\")\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, DEVICE)\n",
    "        print(f\"Train loss: {train_loss:.4f}\")\n",
    "\n",
    "        val_labels, val_logits, _ = validate(model, val_loader, DEVICE)\n",
    "        val_preds = np.argmax(val_logits, axis=1)\n",
    "        # Add zero_division=0 to f1_score to prevent errors if a class has no predictions\n",
    "        f1_macro = f1_score(val_labels, val_preds, average=\"macro\", zero_division=0)\n",
    "        f1_micro = f1_score(val_labels, val_preds, average=\"micro\", zero_division=0)\n",
    "        print(f\"Val F1 macro: {f1_macro:.4f} | F1 micro: {f1_micro:.4f}\")\n",
    "\n",
    "        if f1_macro > best_val_f1:\n",
    "            best_val_f1 = f1_macro\n",
    "            torch.save({\"model_state\": model.state_dict(), \"class_names\": class_names, \"epoch\": epoch}, MODEL_SAVE_PATH)\n",
    "            print(f\"Saved best model to {MODEL_SAVE_PATH} (F1 macro: {best_val_f1:.4f})\")\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    # Test evaluation\n",
    "    print(f\"\\nLoading best model from {MODEL_SAVE_PATH} for test evaluation...\")\n",
    "    if not os.path.exists(MODEL_SAVE_PATH):\n",
    "        print(\"Error: Best model was not saved. Skipping test evaluation.\")\n",
    "        return\n",
    "        \n",
    "    ckpt = torch.load(MODEL_SAVE_PATH, map_location=DEVICE)\n",
    "    model.load_state_dict(ckpt[\"model_state\"])\n",
    "    test_labels, test_logits, test_fns = validate(model, test_loader, DEVICE)\n",
    "    test_preds = np.argmax(test_logits, axis=1)\n",
    "    f1_macro = f1_score(test_labels, test_preds, average=\"macro\", zero_division=0)\n",
    "    f1_micro = f1_score(test_labels, test_preds, average=\"micro\", zero_division=0)\n",
    "    print(f\"Test F1 macro: {f1_macro:.4f} | F1 micro: {f1_micro:.4f}\")\n",
    "\n",
    "    # Save test predictions\n",
    "    test_probs = torch.softmax(torch.tensor(test_logits), dim=1).numpy()\n",
    "    out_df = pd.DataFrame(test_probs, columns=class_names)\n",
    "    out_df[\"filename\"] = test_fns\n",
    "    out_df = out_df[[\"filename\"] + class_names]\n",
    "    out_df.to_csv(\"test_predictions_multiclass.csv\", index=False)\n",
    "    print(\"Saved test_predictions_multiclass.csv\")\n",
    "\n",
    "    # -------------------------\n",
    "    # Example Grad-CAM explanation (replaces previous LIME example)\n",
    "    # -------------------------\n",
    "    print(\"\\nGenerating Grad-CAM example...\")\n",
    "    # We'll produce one Grad-CAM image for a test example matching the first class in class_names (if present).\n",
    "    target_class_name = class_names[0]\n",
    "    target_idx = 0\n",
    "\n",
    "    # Create GradCAM with hook on cnn_features\n",
    "    gradcam = GradCAM(model, target_module_name=\"cnn_features\", device=DEVICE)\n",
    "\n",
    "    # Find a test example of the chosen class (based on ground truth)\n",
    "    found_example = False\n",
    "    if len(test_ds) == 0:\n",
    "        print(\"Test dataset is empty, cannot generate Grad-CAM.\")\n",
    "    else:\n",
    "        for i in range(len(test_ds)):\n",
    "            img_tensor, label_idx, fn = test_ds[i]\n",
    "            if label_idx == target_idx:\n",
    "                # Find the correct full path to the original image\n",
    "                img_path = os.path.join(test_dir, fn)\n",
    "                if not os.path.exists(img_path):\n",
    "                    img_path = os.path.join(test_dir, \"images\", fn)\n",
    "                \n",
    "                if not os.path.exists(img_path):\n",
    "                    print(f\"Warning: Could not find original image {fn} for Grad-CAM. Skipping.\")\n",
    "                    continue\n",
    "\n",
    "                # prepare input batch\n",
    "                input_tensor = img_tensor.unsqueeze(0).to(DEVICE)  # [1,3,H,W]\n",
    "                # generate cam for the true class\n",
    "                cam = gradcam.generate_cam(input_tensor, target_index=target_idx)  # HxW numpy\n",
    "                # read original image for overlay (un-normalize)\n",
    "                orig_pil = Image.open(img_path).convert(\"RGB\").resize((IMAGE_SIZE, IMAGE_SIZE))\n",
    "                orig = np.array(orig_pil).astype(np.uint8)\n",
    "\n",
    "                # create heatmap overlay\n",
    "                cmap = plt.get_cmap(\"jet\")\n",
    "                heatmap = cmap(cam)[:, :, :3]  # HxWx3\n",
    "                heatmap = (heatmap * 255).astype(np.uint8)\n",
    "\n",
    "                overlay = (0.6 * orig.astype(float) + 0.4 * heatmap.astype(float)).astype(np.uint8)\n",
    "\n",
    "                out_img = f\"gradcam_{os.path.basename(fn)}_{target_class_name.strip()}.png\"\n",
    "                plt.imsave(out_img, overlay)\n",
    "                print(\"Saved Grad-CAM visualization:\", out_img)\n",
    "                found_example = True\n",
    "                break\n",
    "        \n",
    "        if not found_example:\n",
    "            print(f\"Could not find a test example for class '{target_class_name}' to generate Grad-CAM.\")\n",
    "\n",
    "    # remove hooks\n",
    "    gradcam.remove_hooks()\n",
    "    print(\"Pipeline complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
