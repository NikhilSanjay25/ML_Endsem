{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8e29d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "fetal_brain_pipeline_multiclass_with_crossattn_gradcam.py\n",
    "\n",
    "Full pipeline (multi-class) â€” updated to include:\n",
    "- Cross-Attention Fusion (CNN <-> ViT)\n",
    "- Grad-CAM explanations (replaces LIME example at end)\n",
    "\n",
    "Other pipeline parts are unchanged: Dataset, CBAM, DenseNet + Swin, training loop, metrics, saving.\n",
    "\n",
    "Author: ChatGPT (integrated into your existing script)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "import timm\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, f1_score, precision_recall_fscore_support\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# -------------------------\n",
    "# Config / hyperparameters\n",
    "# -------------------------\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "DATA_ROOT = \"Classification_Dataset\"\n",
    "BATCH_SIZE = 2\n",
    "IMAGE_SIZE = 224\n",
    "NUM_EPOCHS = 25\n",
    "LR = 1e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "NUM_WORKERS = 0\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MODEL_SAVE_PATH = \"best_model_multiclass.pth\"\n",
    "NUM_CLASSES = 11  # will be overridden after reading CSV if different\n",
    "\n",
    "# -------------------------\n",
    "# Helper: read CSV, get classes\n",
    "# -------------------------\n",
    "def read_classes_from_csv(csv_path: str):\n",
    "    # Use sep=None to auto-detect delimiter, engine='python' for flexibility\n",
    "    df = pd.read_csv(csv_path, sep=None, engine=\"python\")\n",
    "    class_cols = list(df.columns[1:])\n",
    "    return class_cols\n",
    "\n",
    "# -------------------------\n",
    "# Dataset\n",
    "# -------------------------\n",
    "class FetalUSDataset(Dataset):\n",
    "    def __init__(self, split_dir: str, image_size=224, transform=None):\n",
    "        csv_path = os.path.join(split_dir, \"_classes.csv\")\n",
    "        assert os.path.exists(csv_path), f\"CSV not found: {csv_path}\"\n",
    "        self.df = pd.read_csv(csv_path, sep=None, engine=\"python\")\n",
    "        self.dir = split_dir\n",
    "        self.filenames = self.df.iloc[:, 0].astype(str).values\n",
    "        self.labels = self.df.iloc[:, 1:].astype(int).values\n",
    "        self.transform = transform if transform else self.default_transform(image_size)\n",
    "        self.image_size = image_size\n",
    "\n",
    "    def default_transform(self, image_size):\n",
    "        return T.Compose([\n",
    "            T.Resize((image_size, image_size)),\n",
    "            T.Grayscale(num_output_channels=3),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fn = self.filenames[idx]\n",
    "        img_path = os.path.join(self.dir, fn)\n",
    "        if not os.path.exists(img_path):\n",
    "            alt = os.path.join(self.dir, \"images\", fn)\n",
    "            if os.path.exists(alt):\n",
    "                img_path = alt\n",
    "            else:\n",
    "                raise FileNotFoundError(f\"File {fn} not found in {self.dir} or {os.path.join(self.dir, 'images')}\")\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        x = self.transform(img)\n",
    "        y = torch.tensor(np.argmax(self.labels[idx]), dtype=torch.long)  # one-hot -> index\n",
    "        return x, y, fn\n",
    "\n",
    "# -------------------------\n",
    "# CBAM implementation\n",
    "# -------------------------\n",
    "class BasicConv(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, bias=False):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size, stride=stride, padding=padding, bias=bias)\n",
    "        self.bn = nn.BatchNorm2d(out_planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    def forward(self, x):\n",
    "        return self.relu(self.bn(self.conv(x)))\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_planes, ratio=16):\n",
    "        super().__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        self.fc1 = nn.Conv2d(in_planes, max(in_planes // ratio, 1), 1, bias=False)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Conv2d(max(in_planes // ratio, 1), in_planes, 1, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))\n",
    "        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))\n",
    "        return self.sigmoid(avg_out + max_out)\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super().__init__()\n",
    "        padding = 3 if kernel_size==7 else 1\n",
    "        self.conv = BasicConv(2, 1, kernel_size, padding=padding, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x_cat = torch.cat([avg_out, max_out], dim=1)\n",
    "        return self.sigmoid(self.conv(x_cat))\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, in_planes, ratio=16, kernel_size=7):\n",
    "        super().__init__()\n",
    "        self.channel_att = ChannelAttention(in_planes, ratio)\n",
    "        self.spatial_att = SpatialAttention(kernel_size)\n",
    "    def forward(self, x):\n",
    "        x_out = x * self.channel_att(x)\n",
    "        return x_out * self.spatial_att(x_out)\n",
    "\n",
    "# -------------------------\n",
    "# Cross-Attention Fusion module\n",
    "# -------------------------\n",
    "class CrossAttentionFusion(nn.Module):\n",
    "    \"\"\"\n",
    "    Bidirectional cross-attention between CNN spatial tokens and ViT tokens.\n",
    "    - Projects both feature maps to same embed dim\n",
    "    - Flattens spatial dims to token sequences\n",
    "    - Applies MultiheadAttention in both directions\n",
    "    - Pools refined token sequences to vectors for classification\n",
    "    \"\"\"\n",
    "    def __init__(self, cnn_channels, vit_channels, embed_dim=256, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.cnn_proj = nn.Conv2d(cnn_channels, embed_dim, kernel_size=1)\n",
    "        self.vit_proj = nn.Conv2d(vit_channels, embed_dim, kernel_size=1)\n",
    "        self.attn_c2v = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
    "        self.attn_v2c = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
    "        self.norm_c = nn.LayerNorm(embed_dim)\n",
    "        self.norm_v = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, cnn_feat_map, vit_feat_map):\n",
    "        \"\"\"\n",
    "        cnn_feat_map: [B, Cc, Hc, Wc]\n",
    "        vit_feat_map: [B, Cv, Hv, Wv]\n",
    "        returns: cnn_vec [B, E], vit_vec [B, E]\n",
    "        \"\"\"\n",
    "        B = cnn_feat_map.shape[0]\n",
    "        # project\n",
    "        c = self.cnn_proj(cnn_feat_map)   # [B, E, Hc, Wc]\n",
    "        v = self.vit_proj(vit_feat_map)   # [B, E, Hv, Wv]\n",
    "\n",
    "        # flatten spatial dims -> sequences: [B, N, E]\n",
    "        c_seq = c.flatten(2).permute(0, 2, 1).contiguous()  # [B, Hc*Wc, E]\n",
    "        v_seq = v.flatten(2).permute(0, 2, 1).contiguous()  # [B, Hv*Wv, E]\n",
    "\n",
    "        # cross-attention: CNN queries Vit (so keys/values from vit)\n",
    "        # MultiheadAttention batch_first=True accepts (B, N, E)\n",
    "        c_refined, _ = self.attn_c2v(query=c_seq, key=v_seq, value=v_seq)\n",
    "        v_refined, _ = self.attn_v2c(query=v_seq, key=c_seq, value=c_seq)\n",
    "\n",
    "        # LayerNorm\n",
    "        c_refined = self.norm_c(c_refined)\n",
    "        v_refined = self.norm_v(v_refined)\n",
    "\n",
    "        # Pool along token dimension -> vectors\n",
    "        c_vec = c_refined.mean(dim=1)  # [B, E]\n",
    "        v_vec = v_refined.mean(dim=1)  # [B, E]\n",
    "\n",
    "        return c_vec, v_vec\n",
    "\n",
    "# -------------------------\n",
    "# Grad-CAM utility\n",
    "# -------------------------\n",
    "class GradCAM:\n",
    "    \"\"\"\n",
    "    Simple Grad-CAM implementation hooking into the CNN feature map output (cnn_features).\n",
    "    Use generate_cam(input_tensor, target_class) to obtain a heatmap (H_in x W_in).\n",
    "    \"\"\"\n",
    "    def __init__(self, model: nn.Module, target_module_name=\"cnn_features\", device=\"cpu\"):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        self.device = device\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "        \n",
    "        # Find the target module\n",
    "        target_module = None\n",
    "        for name, module in self.model.named_modules():\n",
    "            if name == target_module_name:\n",
    "                target_module = module\n",
    "                break\n",
    "        if target_module is None:\n",
    "             raise ValueError(f\"Target module '{target_module_name}' not found in model.\")\n",
    "             \n",
    "        # forward hook to save activations\n",
    "        def forward_hook(module, input, output):\n",
    "            self.activations = output.detach()\n",
    "        def backward_hook(module, grad_in, grad_out):\n",
    "            # grad_out is a tuple with gradients w.r.t. outputs\n",
    "            self.gradients = grad_out[0].detach()\n",
    "        # register\n",
    "        self.f_hook = target_module.register_forward_hook(forward_hook)\n",
    "        self.b_hook = target_module.register_backward_hook(backward_hook)\n",
    "\n",
    "    def generate_cam(self, input_tensor: torch.Tensor, target_index: int = None):\n",
    "        \"\"\"\n",
    "        input_tensor: single image tensor [1,3,H,W] normalized as model expects\n",
    "        target_index: class index for which to compute Grad-CAM. If None, uses top predicted class.\n",
    "        returns: heatmap numpy array scaled to input HxW (values 0..1)\n",
    "        \"\"\"\n",
    "        self.model.zero_grad()\n",
    "        input_tensor = input_tensor.to(self.device)\n",
    "        logits = self.model(input_tensor)  # [1, C]\n",
    "        if target_index is None:\n",
    "            target_index = torch.argmax(logits, dim=1).item()\n",
    "        score = logits[:, target_index]\n",
    "        score.backward(retain_graph=True)\n",
    "\n",
    "        # get gradients and activations\n",
    "        grads = self.gradients  # [B, Cc, Hc, Wc]\n",
    "        acts = self.activations  # [B, Cc, Hc, Wc]\n",
    "        if grads is None or acts is None:\n",
    "            raise RuntimeError(\"Gradients or activations not captured. Make sure forward() was called on model and hooks are registered.\")\n",
    "\n",
    "        # global-average-pool gradients -> weights [B, Cc]\n",
    "        weights = torch.mean(grads.view(grads.shape[0], grads.shape[1], -1), dim=2)  # [B, Cc]\n",
    "\n",
    "        # weighted combination of activations\n",
    "        # We only operate on the first item in batch (B=1)\n",
    "        cam = torch.zeros(acts.shape[2], acts.shape[3], device=self.device) # Hc, Wc\n",
    "        for i in range(weights.shape[1]): # Cc\n",
    "            cam += weights[0, i] * acts[0, i, :, :]\n",
    "\n",
    "        cam = F.relu(cam)\n",
    "        # normalize cam to [0,1]\n",
    "        cam_min = cam.min()\n",
    "        cam_max = cam.max()\n",
    "        if cam_max > cam_min and cam_max > 0:\n",
    "            cam = (cam - cam_min) / (cam_max - cam_min)\n",
    "        elif cam_max > 0:\n",
    "             cam = cam / cam_max\n",
    "\n",
    "        # upsample to input size\n",
    "        cam = cam.unsqueeze(0).unsqueeze(0)  # [1,1,Hc,Wc]\n",
    "        cam_up = F.interpolate(cam, size=(input_tensor.shape[2], input_tensor.shape[3]), mode='bilinear', align_corners=False)\n",
    "        cam_up = cam_up.squeeze().cpu().numpy()\n",
    "        return cam_up\n",
    "\n",
    "    def remove_hooks(self):\n",
    "        self.f_hook.remove()\n",
    "        self.b_hook.remove()\n",
    "\n",
    "# -------------------------\n",
    "# HybridNet: DenseNet + CBAM + Swin + Cross-Attn Fusion\n",
    "# -------------------------\n",
    "class HybridNet(nn.Module):\n",
    "    def __init__(self, densenet_model_name=\"densenet121\", swin_model_name=\"swin_base_patch4_window7_224\", num_classes=11, image_size=224, device=\"cpu\", cross_embed_dim=256, cross_heads=8):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "        # CNN branch (DenseNet)\n",
    "        dnet = timm.create_model(densenet_model_name, pretrained=True)\n",
    "        self.cnn_features = dnet.features            # will output [B, Cc, Hc, Wc]\n",
    "        cnn_feat_channels = dnet.num_features\n",
    "\n",
    "        # CBAM on CNN feature map\n",
    "        self.cbam = CBAM(cnn_feat_channels)\n",
    "\n",
    "        # global pool for fallback vector (not used for cross-attn fusion)\n",
    "        self.cnn_gap = nn.AdaptiveAvgPool2d((1,1))\n",
    "\n",
    "        # Swin branch\n",
    "        # FIX: global_pool=\"\" to output feature map\n",
    "        self.swin = timm.create_model(swin_model_name, pretrained=True, num_classes=0, global_pool=\"\")\n",
    "        swin_feat_channels = self.swin.num_features\n",
    "        self.swin_pool = nn.AdaptiveAvgPool2d((1,1)) # This is now unused, but harmless\n",
    "\n",
    "        # Cross-attention fusion module\n",
    "        self.cross_fusion = CrossAttentionFusion(cnn_channels=cnn_feat_channels, vit_channels=swin_feat_channels,\n",
    "                                                  embed_dim=cross_embed_dim, num_heads=cross_heads)\n",
    "\n",
    "        # Classifier - input dimension = 2 * cross_embed_dim (cnn_vec + vit_vec)\n",
    "        fusion_in_features = cross_embed_dim * 2\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(fusion_in_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # CNN branch\n",
    "        cnn_feat = self.cnn_features(x)    # [B, Cc, Hc, Wc]\n",
    "        cnn_feat = self.cbam(cnn_feat)     # apply CBAM\n",
    "\n",
    "        # Swin branch\n",
    "        # This outputs [B, H, W, C], e.g. [B, 7, 7, 1024] (channels-last)\n",
    "        swin_feat = self.swin.forward_features(x)\n",
    "\n",
    "        # --- FIX: Permute from channels-last [B, H, W, C] to channels-first [B, C, H, W] ---\n",
    "        # Conv2d layers in cross_fusion expect [B, C, H, W]\n",
    "        swin_feat = swin_feat.permute(0, 3, 1, 2).contiguous() # Shape becomes [B, 1024, 7, 7]\n",
    "        \n",
    "        # Cross-attention fusion -> returns pooled vectors\n",
    "        cnn_vec, vit_vec = self.cross_fusion(cnn_feat, swin_feat)  # each [B, E]\n",
    "\n",
    "        # fuse and classify\n",
    "        fused = torch.cat([cnn_vec, vit_vec], dim=1)  # [B, 2*E]\n",
    "        logits = self.classifier(fused)\n",
    "        return logits\n",
    "\n",
    "# -------------------------\n",
    "# Training / validation (unchanged)\n",
    "# -------------------------\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for imgs, labels, _ in tqdm(loader, desc=\"Train batches\"):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(imgs)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "    return running_loss / len(loader.dataset)\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, loader, device):\n",
    "    model.eval()\n",
    "    all_logits, all_labels, filenames = [], [], []\n",
    "    for imgs, labels, fns in tqdm(loader, desc=\"Validation batches\"):\n",
    "        imgs = imgs.to(device)\n",
    "        logits = model(imgs)\n",
    "        all_logits.append(logits.detach().cpu().numpy())\n",
    "        all_labels.append(labels.numpy())\n",
    "        filenames.extend(fns)\n",
    "    all_logits = np.vstack(all_logits)\n",
    "    all_labels = np.hstack(all_labels)\n",
    "    return all_labels, all_logits, filenames\n",
    "\n",
    "# -------------------------\n",
    "# Main\n",
    "# -------------------------\n",
    "def main():\n",
    "    # splits\n",
    "    train_dir, valid_dir, test_dir = [os.path.join(DATA_ROOT, s) for s in [\"train\",\"valid\",\"test\"]]\n",
    "    \n",
    "    # Ensure train directory and its _classes.csv exist before proceeding\n",
    "    train_csv_path = os.path.join(train_dir, \"_classes.csv\")\n",
    "    if not os.path.exists(train_csv_path):\n",
    "        print(f\"Error: Training CSV not found at {train_csv_path}\")\n",
    "        print(\"Please ensure DATA_ROOT is set correctly and your 'train' folder contains '_classes.csv'\")\n",
    "        return\n",
    "\n",
    "    class_names = read_classes_from_csv(train_csv_path)\n",
    "    global NUM_CLASSES\n",
    "    NUM_CLASSES = len(class_names)\n",
    "    print(\"Classes:\", class_names)\n",
    "    print(f\"Found {NUM_CLASSES} classes.\")\n",
    "\n",
    "    # transforms\n",
    "    train_transform = T.Compose([\n",
    "        T.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "        T.RandomHorizontalFlip(),\n",
    "        T.RandomRotation(10),\n",
    "        T.ColorJitter(0.1,0.1),\n",
    "        T.Grayscale(3),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "    ])\n",
    "    val_transform = T.Compose([\n",
    "        T.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "        T.Grayscale(3),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "    ])\n",
    "\n",
    "    train_ds = FetalUSDataset(train_dir, IMAGE_SIZE, train_transform)\n",
    "    val_ds = FetalUSDataset(valid_dir, IMAGE_SIZE, val_transform)\n",
    "    test_ds = FetalUSDataset(test_dir, IMAGE_SIZE, val_transform)\n",
    "\n",
    "    print(f\"Dataset loaded: {len(train_ds)} train, {len(val_ds)} val, {len(test_ds)} test examples.\")\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "    # model, criterion, optimizer\n",
    "    model = HybridNet(num_classes=NUM_CLASSES, image_size=IMAGE_SIZE, device=DEVICE).to(DEVICE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)\n",
    "\n",
    "    print(f\"Starting training for {NUM_EPOCHS} epochs on {DEVICE}...\")\n",
    "    best_val_f1 = -1.0\n",
    "    for epoch in range(1, NUM_EPOCHS+1):\n",
    "        print(f\"\\nEpoch {epoch}/{NUM_EPOCHS}\")\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, DEVICE)\n",
    "        print(f\"Train loss: {train_loss:.4f}\")\n",
    "\n",
    "        val_labels, val_logits, _ = validate(model, val_loader, DEVICE)\n",
    "        val_preds = np.argmax(val_logits, axis=1)\n",
    "        # Add zero_division=0 to f1_score to prevent errors if a class has no predictions\n",
    "        f1_macro = f1_score(val_labels, val_preds, average=\"macro\", zero_division=0)\n",
    "        f1_micro = f1_score(val_labels, val_preds, average=\"micro\", zero_division=0)\n",
    "        print(f\"Val F1 macro: {f1_macro:.4f} | F1 micro: {f1_micro:.4f}\")\n",
    "\n",
    "        if f1_macro > best_val_f1:\n",
    "            best_val_f1 = f1_macro\n",
    "            torch.save({\"model_state\": model.state_dict(), \"class_names\": class_names, \"epoch\": epoch}, MODEL_SAVE_PATH)\n",
    "            print(f\"Saved best model to {MODEL_SAVE_PATH} (F1 macro: {best_val_f1:.4f})\")\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    # Test evaluation\n",
    "    print(f\"\\nLoading best model from {MODEL_SAVE_PATH} for test evaluation...\")\n",
    "    if not os.path.exists(MODEL_SAVE_PATH):\n",
    "        print(\"Error: Best model was not saved. Skipping test evaluation.\")\n",
    "        return\n",
    "        \n",
    "    ckpt = torch.load(MODEL_SAVE_PATH, map_location=DEVICE)\n",
    "    model.load_state_dict(ckpt[\"model_state\"])\n",
    "    test_labels, test_logits, test_fns = validate(model, test_loader, DEVICE)\n",
    "    test_preds = np.argmax(test_logits, axis=1)\n",
    "    f1_macro = f1_score(test_labels, test_preds, average=\"macro\", zero_division=0)\n",
    "    f1_micro = f1_score(test_labels, test_preds, average=\"micro\", zero_division=0)\n",
    "    print(f\"Test F1 macro: {f1_macro:.4f} | F1 micro: {f1_micro:.4f}\")\n",
    "\n",
    "    # Save test predictions\n",
    "    test_probs = torch.softmax(torch.tensor(test_logits), dim=1).numpy()\n",
    "    out_df = pd.DataFrame(test_probs, columns=class_names)\n",
    "    out_df[\"filename\"] = test_fns\n",
    "    out_df = out_df[[\"filename\"] + class_names]\n",
    "    out_df.to_csv(\"test_predictions_multiclass.csv\", index=False)\n",
    "    print(\"Saved test_predictions_multiclass.csv\")\n",
    "\n",
    "    # -------------------------\n",
    "    # Example Grad-CAM explanation (replaces previous LIME example)\n",
    "    # -------------------------\n",
    "    print(\"\\nGenerating Grad-CAM example...\")\n",
    "    # We'll produce one Grad-CAM image for a test example matching the first class in class_names (if present).\n",
    "    target_class_name = class_names[0]\n",
    "    target_idx = 0\n",
    "\n",
    "    # Create GradCAM with hook on cnn_features\n",
    "    gradcam = GradCAM(model, target_module_name=\"cnn_features\", device=DEVICE)\n",
    "\n",
    "    # Find a test example of the chosen class (based on ground truth)\n",
    "    found_example = False\n",
    "    if len(test_ds) == 0:\n",
    "        print(\"Test dataset is empty, cannot generate Grad-CAM.\")\n",
    "    else:\n",
    "        for i in range(len(test_ds)):\n",
    "            img_tensor, label_idx, fn = test_ds[i]\n",
    "            if label_idx == target_idx:\n",
    "                # Find the correct full path to the original image\n",
    "                img_path = os.path.join(test_dir, fn)\n",
    "                if not os.path.exists(img_path):\n",
    "                    img_path = os.path.join(test_dir, \"images\", fn)\n",
    "                \n",
    "                if not os.path.exists(img_path):\n",
    "                    print(f\"Warning: Could not find original image {fn} for Grad-CAM. Skipping.\")\n",
    "                    continue\n",
    "\n",
    "                # prepare input batch\n",
    "                input_tensor = img_tensor.unsqueeze(0).to(DEVICE)  # [1,3,H,W]\n",
    "                # generate cam for the true class\n",
    "                cam = gradcam.generate_cam(input_tensor, target_index=target_idx)  # HxW numpy\n",
    "                # read original image for overlay (un-normalize)\n",
    "                orig_pil = Image.open(img_path).convert(\"RGB\").resize((IMAGE_SIZE, IMAGE_SIZE))\n",
    "                orig = np.array(orig_pil).astype(np.uint8)\n",
    "\n",
    "                # create heatmap overlay\n",
    "                cmap = plt.get_cmap(\"jet\")\n",
    "                heatmap = cmap(cam)[:, :, :3]  # HxWx3\n",
    "                heatmap = (heatmap * 255).astype(np.uint8)\n",
    "\n",
    "                overlay = (0.6 * orig.astype(float) + 0.4 * heatmap.astype(float)).astype(np.uint8)\n",
    "\n",
    "                out_img = f\"gradcam_{os.path.basename(fn)}_{target_class_name.strip()}.png\"\n",
    "                plt.imsave(out_img, overlay)\n",
    "                print(\"Saved Grad-CAM visualization:\", out_img)\n",
    "                found_example = True\n",
    "                break\n",
    "        \n",
    "        if not found_example:\n",
    "            print(f\"Could not find a test example for class '{target_class_name}' to generate Grad-CAM.\")\n",
    "\n",
    "    # remove hooks\n",
    "    gradcam.remove_hooks()\n",
    "    print(\"Pipeline complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be40668c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nikhi\\OneDrive\\Attachments\\Desktop\\Git_Repos\\ML_Endsem\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: [' anold-chiari-malformation', ' arachnoid-cyst', ' cerebellah-hypoplasia', ' colphocephaly', ' encephalocele', ' holoprosencephaly', ' hydracenphaly', ' intracranial-hemorrdge', ' intracranial-tumor', ' m-magna', ' mild-ventriculomegaly', ' moderate-ventriculomegaly', ' normal', ' polencephaly', ' severe-ventriculomegaly', ' vein-of-galen']\n",
      "Found 16 classes.\n",
      "Dataset loaded: 1413 train, 176 val, 179 test examples.\n",
      "Starting training for 25 epochs on cuda...\n",
      "\n",
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 707/707 [02:08<00:00,  5.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 2.3719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 88/88 [00:05<00:00, 16.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 2.1176 | Val F1 macro: 0.0775 | F1 micro: 0.3182\n",
      "Saved best model to best_model_multiclass.pth (F1 macro: 0.0775)\n",
      "\n",
      "Epoch 2/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 707/707 [01:39<00:00,  7.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 2.0216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 88/88 [00:03<00:00, 25.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 1.7242 | Val F1 macro: 0.1596 | F1 micro: 0.4375\n",
      "Saved best model to best_model_multiclass.pth (F1 macro: 0.1596)\n",
      "\n",
      "Epoch 3/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 707/707 [01:38<00:00,  7.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.6841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 88/88 [00:03<00:00, 25.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 1.4242 | Val F1 macro: 0.3112 | F1 micro: 0.5625\n",
      "Saved best model to best_model_multiclass.pth (F1 macro: 0.3112)\n",
      "\n",
      "Epoch 4/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 707/707 [01:44<00:00,  6.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.3250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 88/88 [00:04<00:00, 20.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 1.3398 | Val F1 macro: 0.3156 | F1 micro: 0.5625\n",
      "Saved best model to best_model_multiclass.pth (F1 macro: 0.3156)\n",
      "\n",
      "Epoch 5/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 707/707 [01:44<00:00,  6.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.9729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 88/88 [00:03<00:00, 22.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 1.0581 | Val F1 macro: 0.4830 | F1 micro: 0.7273\n",
      "Saved best model to best_model_multiclass.pth (F1 macro: 0.4830)\n",
      "\n",
      "Epoch 6/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 707/707 [01:46<00:00,  6.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.8135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 88/88 [00:03<00:00, 22.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.9331 | Val F1 macro: 0.5487 | F1 micro: 0.7500\n",
      "Saved best model to best_model_multiclass.pth (F1 macro: 0.5487)\n",
      "\n",
      "Epoch 7/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 707/707 [01:44<00:00,  6.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.6482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 88/88 [00:03<00:00, 22.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.9497 | Val F1 macro: 0.5967 | F1 micro: 0.7898\n",
      "Saved best model to best_model_multiclass.pth (F1 macro: 0.5967)\n",
      "\n",
      "Epoch 8/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 707/707 [01:45<00:00,  6.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 88/88 [00:03<00:00, 23.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.5974 | Val F1 macro: 0.7180 | F1 micro: 0.8466\n",
      "Saved best model to best_model_multiclass.pth (F1 macro: 0.7180)\n",
      "\n",
      "Epoch 9/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 707/707 [01:45<00:00,  6.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.3829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 88/88 [00:03<00:00, 23.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.5918 | Val F1 macro: 0.6011 | F1 micro: 0.8239\n",
      "\n",
      "Epoch 10/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 707/707 [01:42<00:00,  6.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.3185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 88/88 [00:03<00:00, 25.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.5592 | Val F1 macro: 0.6675 | F1 micro: 0.8352\n",
      "\n",
      "Epoch 11/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 707/707 [01:38<00:00,  7.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 88/88 [00:03<00:00, 25.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.5328 | Val F1 macro: 0.7364 | F1 micro: 0.8750\n",
      "Saved best model to best_model_multiclass.pth (F1 macro: 0.7364)\n",
      "\n",
      "Epoch 12/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 707/707 [01:38<00:00,  7.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 88/88 [00:03<00:00, 25.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.4113 | Val F1 macro: 0.8321 | F1 micro: 0.9148\n",
      "Saved best model to best_model_multiclass.pth (F1 macro: 0.8321)\n",
      "\n",
      "Epoch 13/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 707/707 [01:38<00:00,  7.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 88/88 [00:03<00:00, 25.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.4102 | Val F1 macro: 0.8262 | F1 micro: 0.9091\n",
      "\n",
      "Epoch 14/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 707/707 [01:37<00:00,  7.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 88/88 [00:03<00:00, 25.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.4917 | Val F1 macro: 0.7747 | F1 micro: 0.8807\n",
      "\n",
      "Epoch 15/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 707/707 [01:37<00:00,  7.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 88/88 [00:03<00:00, 25.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.5765 | Val F1 macro: 0.7506 | F1 micro: 0.8864\n",
      "\n",
      "Epoch 16/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 707/707 [01:38<00:00,  7.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.1267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 88/88 [00:03<00:00, 25.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.4836 | Val F1 macro: 0.8107 | F1 micro: 0.8977\n",
      "\n",
      "Epoch 17/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 707/707 [01:37<00:00,  7.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 88/88 [00:03<00:00, 25.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.5034 | Val F1 macro: 0.8191 | F1 micro: 0.8864\n",
      "\n",
      "Epoch 18/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 707/707 [01:43<00:00,  6.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 88/88 [00:03<00:00, 22.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.5441 | Val F1 macro: 0.7585 | F1 micro: 0.8523\n",
      "\n",
      "Epoch 19/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 707/707 [02:08<00:00,  5.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 88/88 [00:06<00:00, 14.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.4202 | Val F1 macro: 0.7511 | F1 micro: 0.9148\n",
      "\n",
      "Epoch 20/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 707/707 [02:17<00:00,  5.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 88/88 [00:06<00:00, 13.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.3555 | Val F1 macro: 0.8174 | F1 micro: 0.9205\n",
      "\n",
      "Epoch 21/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 707/707 [02:36<00:00,  4.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 88/88 [00:03<00:00, 22.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.3991 | Val F1 macro: 0.8255 | F1 micro: 0.9148\n",
      "\n",
      "Epoch 22/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 707/707 [01:55<00:00,  6.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 88/88 [00:06<00:00, 14.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.2887 | Val F1 macro: 0.8303 | F1 micro: 0.9205\n",
      "\n",
      "Epoch 23/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 707/707 [01:40<00:00,  7.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 88/88 [00:03<00:00, 24.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.4113 | Val F1 macro: 0.8135 | F1 micro: 0.9205\n",
      "\n",
      "Epoch 24/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 707/707 [01:37<00:00,  7.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 88/88 [00:03<00:00, 25.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.3426 | Val F1 macro: 0.8141 | F1 micro: 0.9205\n",
      "\n",
      "Epoch 25/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 707/707 [01:43<00:00,  6.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 88/88 [00:03<00:00, 22.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.3649 | Val F1 macro: 0.8659 | F1 micro: 0.9375\n",
      "Saved best model to best_model_multiclass.pth (F1 macro: 0.8659)\n",
      "\n",
      "Training complete. Plotting metrics...\n",
      "Saved training metrics plot to training_metrics.png\n",
      "\n",
      "Loading best model from best_model_multiclass.pth for test evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikhi\\AppData\\Local\\Temp\\ipykernel_5100\\1812816559.py:529: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(MODEL_SAVE_PATH, map_location=DEVICE)\n",
      "Validation batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [00:05<00:00, 17.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1 macro: 0.8622 | F1 micro: 0.8994\n",
      "\n",
      "Generating Classification Report...\n",
      "                            precision    recall  f1-score   support\n",
      "\n",
      " anold-chiari-malformation       0.75      1.00      0.86         3\n",
      "            arachnoid-cyst       1.00      1.00      1.00        13\n",
      "     cerebellah-hypoplasia       0.75      0.75      0.75        16\n",
      "             colphocephaly       0.90      0.82      0.86        11\n",
      "             encephalocele       1.00      0.93      0.97        15\n",
      "         holoprosencephaly       1.00      1.00      1.00         4\n",
      "             hydracenphaly       1.00      1.00      1.00         2\n",
      "    intracranial-hemorrdge       1.00      1.00      1.00         8\n",
      "        intracranial-tumor       1.00      1.00      1.00         2\n",
      "                   m-magna       0.80      1.00      0.89         4\n",
      "     mild-ventriculomegaly       0.96      0.92      0.94        24\n",
      " moderate-ventriculomegaly       0.87      0.77      0.82        26\n",
      "                    normal       0.85      0.96      0.90        24\n",
      "              polencephaly       0.78      1.00      0.88         7\n",
      "   severe-ventriculomegaly       0.95      0.95      0.95        19\n",
      "             vein-of-galen       0.00      0.00      0.00         1\n",
      "\n",
      "                  accuracy                           0.90       179\n",
      "                 macro avg       0.85      0.88      0.86       179\n",
      "              weighted avg       0.90      0.90      0.90       179\n",
      "\n",
      "Saved classification_report.txt\n",
      "\n",
      "Generating Confusion Matrix...\n",
      "Saved confusion matrix to test_confusion_matrix.png\n",
      "Saved test_predictions_multiclass.csv\n",
      "\n",
      "Generating Grad-CAM example...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nikhi\\OneDrive\\Attachments\\Desktop\\Git_Repos\\ML_Endsem\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Grad-CAM side-by-side plot: gradcam_Copy-of-anold-chiari-malformation-26b_aug_2_png_jpg.rf.25de018d6b186681e0643aea0ec8511f.jpg_anold-chiari-malformation.png\n",
      "Pipeline complete.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "fetal_brain_pipeline_multiclass_with_crossattn_gradcam.py\n",
    "\n",
    "Full pipeline (multi-class) â€” updated to include:\n",
    "- Cross-Attention Fusion (CNN <-> ViT)\n",
    "- Grad-CAM explanations (replaces LIME example at end)\n",
    "- Enhanced Data Augmentation\n",
    "- Plotting for loss/F1 curves and confusion matrix\n",
    "- Detailed classification report\n",
    "\n",
    "Author: ChatGPT (integrated into your existing script)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "import timm\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, \n",
    "    f1_score, \n",
    "    precision_recall_fscore_support,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay\n",
    ")\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# -------------------------\n",
    "# Config / hyperparameters\n",
    "# -------------------------\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "DATA_ROOT = \"Classification_Dataset\"\n",
    "BATCH_SIZE = 2\n",
    "IMAGE_SIZE = 224\n",
    "NUM_EPOCHS = 25\n",
    "LR = 1e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "NUM_WORKERS = 0\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MODEL_SAVE_PATH = \"best_model_multiclass.pth\"\n",
    "NUM_CLASSES = 11  # will be overridden after reading CSV if different\n",
    "\n",
    "# -------------------------\n",
    "# Helper: read CSV, get classes\n",
    "# -------------------------\n",
    "def read_classes_from_csv(csv_path: str):\n",
    "    # Use sep=None to auto-detect delimiter, engine='python' for flexibility\n",
    "    df = pd.read_csv(csv_path, sep=None, engine=\"python\")\n",
    "    class_cols = list(df.columns[1:])\n",
    "    return class_cols\n",
    "\n",
    "# -------------------------\n",
    "# Dataset\n",
    "# -------------------------\n",
    "class FetalUSDataset(Dataset):\n",
    "    def __init__(self, split_dir: str, image_size=224, transform=None):\n",
    "        csv_path = os.path.join(split_dir, \"_classes.csv\")\n",
    "        assert os.path.exists(csv_path), f\"CSV not found: {csv_path}\"\n",
    "        self.df = pd.read_csv(csv_path, sep=None, engine=\"python\")\n",
    "        self.dir = split_dir\n",
    "        self.filenames = self.df.iloc[:, 0].astype(str).values\n",
    "        self.labels = self.df.iloc[:, 1:].astype(int).values\n",
    "        self.transform = transform if transform else self.default_transform(image_size)\n",
    "        self.image_size = image_size\n",
    "\n",
    "    def default_transform(self, image_size):\n",
    "        return T.Compose([\n",
    "            T.Resize((image_size, image_size)),\n",
    "            T.Grayscale(num_output_channels=3),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fn = self.filenames[idx]\n",
    "        img_path = os.path.join(self.dir, fn)\n",
    "        if not os.path.exists(img_path):\n",
    "            alt = os.path.join(self.dir, \"images\", fn)\n",
    "            if os.path.exists(alt):\n",
    "                img_path = alt\n",
    "            else:\n",
    "                raise FileNotFoundError(f\"File {fn} not found in {self.dir} or {os.path.join(self.dir, 'images')}\")\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        x = self.transform(img)\n",
    "        y = torch.tensor(np.argmax(self.labels[idx]), dtype=torch.long)  # one-hot -> index\n",
    "        return x, y, fn\n",
    "\n",
    "# -------------------------\n",
    "# CBAM implementation\n",
    "# -------------------------\n",
    "class BasicConv(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, bias=False):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size, stride=stride, padding=padding, bias=bias)\n",
    "        self.bn = nn.BatchNorm2d(out_planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    def forward(self, x):\n",
    "        return self.relu(self.bn(self.conv(x)))\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, in_planes, ratio=16):\n",
    "        super().__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        self.fc1 = nn.Conv2d(in_planes, max(in_planes // ratio, 1), 1, bias=False)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Conv2d(max(in_planes // ratio, 1), in_planes, 1, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))\n",
    "        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))\n",
    "        return self.sigmoid(avg_out + max_out)\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super().__init__()\n",
    "        padding = 3 if kernel_size==7 else 1\n",
    "        self.conv = BasicConv(2, 1, kernel_size, padding=padding, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        x_cat = torch.cat([avg_out, max_out], dim=1)\n",
    "        return self.sigmoid(self.conv(x_cat))\n",
    "\n",
    "class CBAM(nn.Module):\n",
    "    def __init__(self, in_planes, ratio=16, kernel_size=7):\n",
    "        super().__init__()\n",
    "        self.channel_att = ChannelAttention(in_planes, ratio)\n",
    "        self.spatial_att = SpatialAttention(kernel_size)\n",
    "    def forward(self, x):\n",
    "        x_out = x * self.channel_att(x)\n",
    "        return x_out * self.spatial_att(x_out)\n",
    "\n",
    "# -------------------------\n",
    "# Cross-Attention Fusion module\n",
    "# -------------------------\n",
    "class CrossAttentionFusion(nn.Module):\n",
    "    \"\"\"\n",
    "    Bidirectional cross-attention between CNN spatial tokens and ViT tokens.\n",
    "    - Projects both feature maps to same embed dim\n",
    "    - Flattens spatial dims to token sequences\n",
    "    - Applies MultiheadAttention in both directions\n",
    "    - Pools refined token sequences to vectors for classification\n",
    "    \"\"\"\n",
    "    def __init__(self, cnn_channels, vit_channels, embed_dim=256, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.cnn_proj = nn.Conv2d(cnn_channels, embed_dim, kernel_size=1)\n",
    "        self.vit_proj = nn.Conv2d(vit_channels, embed_dim, kernel_size=1)\n",
    "        self.attn_c2v = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
    "        self.attn_v2c = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
    "        self.norm_c = nn.LayerNorm(embed_dim)\n",
    "        self.norm_v = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, cnn_feat_map, vit_feat_map):\n",
    "        \"\"\"\n",
    "        cnn_feat_map: [B, Cc, Hc, Wc]\n",
    "        vit_feat_map: [B, Cv, Hv, Wv]\n",
    "        returns: cnn_vec [B, E], vit_vec [B, E]\n",
    "        \"\"\"\n",
    "        B = cnn_feat_map.shape[0]\n",
    "        # project\n",
    "        c = self.cnn_proj(cnn_feat_map)  # [B, E, Hc, Wc]\n",
    "        v = self.vit_proj(vit_feat_map)  # [B, E, Hv, Wv]\n",
    "\n",
    "        # flatten spatial dims -> sequences: [B, N, E]\n",
    "        c_seq = c.flatten(2).permute(0, 2, 1).contiguous()  # [B, Hc*Wc, E]\n",
    "        v_seq = v.flatten(2).permute(0, 2, 1).contiguous()  # [B, Hv*Wv, E]\n",
    "\n",
    "        # cross-attention: CNN queries Vit (so keys/values from vit)\n",
    "        # MultiheadAttention batch_first=True accepts (B, N, E)\n",
    "        c_refined, _ = self.attn_c2v(query=c_seq, key=v_seq, value=v_seq)\n",
    "        v_refined, _ = self.attn_v2c(query=v_seq, key=c_seq, value=c_seq)\n",
    "\n",
    "        # LayerNorm\n",
    "        c_refined = self.norm_c(c_refined)\n",
    "        v_refined = self.norm_v(v_refined)\n",
    "\n",
    "        # Pool along token dimension -> vectors\n",
    "        c_vec = c_refined.mean(dim=1)  # [B, E]\n",
    "        v_vec = v_refined.mean(dim=1)  # [B, E]\n",
    "\n",
    "        return c_vec, v_vec\n",
    "\n",
    "# -------------------------\n",
    "# Grad-CAM utility\n",
    "# -------------------------\n",
    "class GradCAM:\n",
    "    \"\"\"\n",
    "    Simple Grad-CAM implementation hooking into the CNN feature map output (cnn_features).\n",
    "    Use generate_cam(input_tensor, target_class) to obtain a heatmap (H_in x W_in).\n",
    "    \"\"\"\n",
    "    def __init__(self, model: nn.Module, target_module_name=\"cnn_features\", device=\"cpu\"):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        self.device = device\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "        \n",
    "        # Find the target module\n",
    "        target_module = None\n",
    "        for name, module in self.model.named_modules():\n",
    "            if name == target_module_name:\n",
    "                target_module = module\n",
    "                break\n",
    "        if target_module is None:\n",
    "              raise ValueError(f\"Target module '{target_module_name}' not found in model.\")\n",
    "            \n",
    "        # forward hook to save activations\n",
    "        def forward_hook(module, input, output):\n",
    "            self.activations = output.detach()\n",
    "        def backward_hook(module, grad_in, grad_out):\n",
    "            # grad_out is a tuple with gradients w.r.t. outputs\n",
    "            self.gradients = grad_out[0].detach()\n",
    "        # register\n",
    "        self.f_hook = target_module.register_forward_hook(forward_hook)\n",
    "        self.b_hook = target_module.register_backward_hook(backward_hook)\n",
    "\n",
    "    def generate_cam(self, input_tensor: torch.Tensor, target_index: int = None):\n",
    "        \"\"\"\n",
    "        input_tensor: single image tensor [1,3,H,W] normalized as model expects\n",
    "        target_index: class index for which to compute Grad-CAM. If None, uses top predicted class.\n",
    "        returns: heatmap numpy array scaled to input HxW (values 0..1)\n",
    "        \"\"\"\n",
    "        self.model.zero_grad()\n",
    "        input_tensor = input_tensor.to(self.device)\n",
    "        logits = self.model(input_tensor)  # [1, C]\n",
    "        if target_index is None:\n",
    "            target_index = torch.argmax(logits, dim=1).item()\n",
    "        score = logits[:, target_index]\n",
    "        score.backward(retain_graph=True)\n",
    "\n",
    "        # get gradients and activations\n",
    "        grads = self.gradients  # [B, Cc, Hc, Wc]\n",
    "        acts = self.activations  # [B, Cc, Hc, Wc]\n",
    "        if grads is None or acts is None:\n",
    "            raise RuntimeError(\"Gradients or activations not captured. Make sure forward() was called on model and hooks are registered.\")\n",
    "\n",
    "        # global-average-pool gradients -> weights [B, Cc]\n",
    "        weights = torch.mean(grads.view(grads.shape[0], grads.shape[1], -1), dim=2)  # [B, Cc]\n",
    "\n",
    "        # weighted combination of activations\n",
    "        # We only operate on the first item in batch (B=1)\n",
    "        cam = torch.zeros(acts.shape[2], acts.shape[3], device=self.device) # Hc, Wc\n",
    "        for i in range(weights.shape[1]): # Cc\n",
    "            cam += weights[0, i] * acts[0, i, :, :]\n",
    "\n",
    "        cam = F.relu(cam)\n",
    "        # normalize cam to [0,1]\n",
    "        cam_min = cam.min()\n",
    "        cam_max = cam.max()\n",
    "        if cam_max > cam_min and cam_max > 0:\n",
    "            cam = (cam - cam_min) / (cam_max - cam_min)\n",
    "        elif cam_max > 0:\n",
    "             cam = cam / cam_max\n",
    "\n",
    "        # upsample to input size\n",
    "        cam = cam.unsqueeze(0).unsqueeze(0)  # [1,1,Hc,Wc]\n",
    "        cam_up = F.interpolate(cam, size=(input_tensor.shape[2], input_tensor.shape[3]), mode='bilinear', align_corners=False)\n",
    "        cam_up = cam_up.squeeze().cpu().numpy()\n",
    "        return cam_up\n",
    "\n",
    "    def remove_hooks(self):\n",
    "        self.f_hook.remove()\n",
    "        self.b_hook.remove()\n",
    "\n",
    "# -------------------------\n",
    "# HybridNet: DenseNet + CBAM + Swin + Cross-Attn Fusion\n",
    "# -------------------------\n",
    "class HybridNet(nn.Module):\n",
    "    def __init__(self, densenet_model_name=\"densenet121\", swin_model_name=\"swin_base_patch4_window7_224\", num_classes=11, image_size=224, device=\"cpu\", cross_embed_dim=256, cross_heads=8):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "\n",
    "        # CNN branch (DenseNet)\n",
    "        dnet = timm.create_model(densenet_model_name, pretrained=True)\n",
    "        self.cnn_features = dnet.features        # will output [B, Cc, Hc, Wc]\n",
    "        cnn_feat_channels = dnet.num_features\n",
    "\n",
    "        # CBAM on CNN feature map\n",
    "        self.cbam = CBAM(cnn_feat_channels)\n",
    "\n",
    "        # global pool for fallback vector (not used for cross-attn fusion)\n",
    "        self.cnn_gap = nn.AdaptiveAvgPool2d((1,1))\n",
    "\n",
    "        # Swin branch\n",
    "        # FIX: global_pool=\"\" to output feature map\n",
    "        self.swin = timm.create_model(swin_model_name, pretrained=True, num_classes=0, global_pool=\"\")\n",
    "        swin_feat_channels = self.swin.num_features\n",
    "        self.swin_pool = nn.AdaptiveAvgPool2d((1,1)) # This is now unused, but harmless\n",
    "\n",
    "        # Cross-attention fusion module\n",
    "        self.cross_fusion = CrossAttentionFusion(cnn_channels=cnn_feat_channels, vit_channels=swin_feat_channels,\n",
    "                                                embed_dim=cross_embed_dim, num_heads=cross_heads)\n",
    "\n",
    "        # Classifier - input dimension = 2 * cross_embed_dim (cnn_vec + vit_vec)\n",
    "        fusion_in_features = cross_embed_dim * 2\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(fusion_in_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # CNN branch\n",
    "        cnn_feat = self.cnn_features(x)    # [B, Cc, Hc, Wc]\n",
    "        cnn_feat = self.cbam(cnn_feat)     # apply CBAM\n",
    "\n",
    "        # Swin branch\n",
    "        # This outputs [B, H, W, C], e.g. [B, 7, 7, 1024] (channels-last)\n",
    "        swin_feat = self.swin.forward_features(x)\n",
    "\n",
    "        # --- FIX: Permute from channels-last [B, H, W, C] to channels-first [B, C, H, W] ---\n",
    "        # Conv2d layers in cross_fusion expect [B, C, H, W]\n",
    "        swin_feat = swin_feat.permute(0, 3, 1, 2).contiguous() # Shape becomes [B, 1024, 7, 7]\n",
    "        \n",
    "        # Cross-attention fusion -> returns pooled vectors\n",
    "        cnn_vec, vit_vec = self.cross_fusion(cnn_feat, swin_feat)  # each [B, E]\n",
    "\n",
    "        # fuse and classify\n",
    "        fused = torch.cat([cnn_vec, vit_vec], dim=1)  # [B, 2*E]\n",
    "        logits = self.classifier(fused)\n",
    "        return logits\n",
    "\n",
    "# -------------------------\n",
    "# Training / validation\n",
    "# -------------------------\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for imgs, labels, _ in tqdm(loader, desc=\"Train batches\"):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(imgs)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "    return running_loss / len(loader.dataset)\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_logits, all_labels, filenames = [], [], []\n",
    "    for imgs, labels, fns in tqdm(loader, desc=\"Validation batches\"):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        logits = model(imgs)\n",
    "        loss = criterion(logits, labels)\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "        \n",
    "        all_logits.append(logits.detach().cpu().numpy())\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "        filenames.extend(fns)\n",
    "        \n",
    "    all_logits = np.vstack(all_logits)\n",
    "    all_labels = np.hstack(all_labels)\n",
    "    val_loss = running_loss / len(loader.dataset)\n",
    "    return val_loss, all_labels, all_logits, filenames\n",
    "\n",
    "# -------------------------\n",
    "# Plotting Helpers\n",
    "# -------------------------\n",
    "def plot_metrics(train_losses, val_losses, val_f1_macros):\n",
    "    \"\"\"Saves a plot of training/validation loss and validation F1 score.\"\"\"\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Plot 1: Loss\n",
    "    ax1.plot(epochs, train_losses, 'b-o', label='Training Loss')\n",
    "    ax1.plot(epochs, val_losses, 'r-o', label='Validation Loss')\n",
    "    ax1.set_title('Training and Validation Loss')\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Plot 2: F1 Macro\n",
    "    ax2.plot(epochs, val_f1_macros, 'g-o', label='Validation F1 Macro')\n",
    "    ax2.set_title('Validation F1 Macro Score')\n",
    "    ax2.set_xlabel('Epochs')\n",
    "    ax2.set_ylabel('F1 Macro')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"training_metrics.png\")\n",
    "    print(\"Saved training metrics plot to training_metrics.png\")\n",
    "    plt.close(fig)\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, class_names, save_path=\"confusion_matrix.png\"):\n",
    "    \"\"\"Saves a plot of the confusion matrix.\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Calculate figure size dynamically\n",
    "    num_classes = len(class_names)\n",
    "    fig_size = max(8, num_classes * 0.8) # Adjust 0.8 to scale\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(fig_size, fig_size))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "    \n",
    "    disp.plot(ax=ax, cmap=plt.cm.Blues, xticks_rotation=90)\n",
    "    \n",
    "    ax.set_title(\"Confusion Matrix\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    print(f\"Saved confusion matrix to {save_path}\")\n",
    "    plt.close(fig)\n",
    "\n",
    "# -------------------------\n",
    "# Main\n",
    "# -------------------------\n",
    "def main():\n",
    "    # splits\n",
    "    train_dir, valid_dir, test_dir = [os.path.join(DATA_ROOT, s) for s in [\"train\",\"valid\",\"test\"]]\n",
    "    \n",
    "    # Ensure train directory and its _classes.csv exist before proceeding\n",
    "    train_csv_path = os.path.join(train_dir, \"_classes.csv\")\n",
    "    if not os.path.exists(train_csv_path):\n",
    "        print(f\"Error: Training CSV not found at {train_csv_path}\")\n",
    "        print(\"Please ensure DATA_ROOT is set correctly and your 'train' folder contains '_classes.csv'\")\n",
    "        return\n",
    "\n",
    "    class_names = read_classes_from_csv(train_csv_path)\n",
    "    global NUM_CLASSES\n",
    "    NUM_CLASSES = len(class_names)\n",
    "    print(\"Classes:\", class_names)\n",
    "    print(f\"Found {NUM_CLASSES} classes.\")\n",
    "\n",
    "    # transforms\n",
    "    # --- UPDATED: Enhanced Data Augmentation ---\n",
    "    train_transform = T.Compose([\n",
    "        T.RandomResizedCrop(IMAGE_SIZE, scale=(0.85, 1.0)),\n",
    "        T.RandomHorizontalFlip(),\n",
    "        T.RandomRotation(15),\n",
    "        T.RandomAffine(degrees=0, translate=(0.1, 0.1), shear=10),\n",
    "        T.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
    "        T.Grayscale(3),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "    ])\n",
    "    val_transform = T.Compose([\n",
    "        T.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "        T.Grayscale(3),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "    ])\n",
    "\n",
    "    train_ds = FetalUSDataset(train_dir, IMAGE_SIZE, train_transform)\n",
    "    val_ds = FetalUSDataset(valid_dir, IMAGE_SIZE, val_transform)\n",
    "    test_ds = FetalUSDataset(test_dir, IMAGE_SIZE, val_transform)\n",
    "\n",
    "    print(f\"Dataset loaded: {len(train_ds)} train, {len(val_ds)} val, {len(test_ds)} test examples.\")\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "    # model, criterion, optimizer\n",
    "    model = HybridNet(num_classes=NUM_CLASSES, image_size=IMAGE_SIZE, device=DEVICE).to(DEVICE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)\n",
    "\n",
    "    print(f\"Starting training for {NUM_EPOCHS} epochs on {DEVICE}...\")\n",
    "    best_val_f1 = -1.0\n",
    "    \n",
    "    # --- NEW: Lists to store metrics for plotting ---\n",
    "    train_losses, val_losses, val_f1_macros = [], [], []\n",
    "\n",
    "    for epoch in range(1, NUM_EPOCHS+1):\n",
    "        print(f\"\\nEpoch {epoch}/{NUM_EPOCHS}\")\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, DEVICE)\n",
    "        print(f\"Train loss: {train_loss:.4f}\")\n",
    "\n",
    "        # --- UPDATED: Validate now returns loss ---\n",
    "        val_loss, val_labels, val_logits, _ = validate(model, val_loader, criterion, DEVICE)\n",
    "        val_preds = np.argmax(val_logits, axis=1)\n",
    "        # Add zero_division=0 to f1_score to prevent errors if a class has no predictions\n",
    "        f1_macro = f1_score(val_labels, val_preds, average=\"macro\", zero_division=0)\n",
    "        f1_micro = f1_score(val_labels, val_preds, average=\"micro\", zero_division=0)\n",
    "        print(f\"Val loss: {val_loss:.4f} | Val F1 macro: {f1_macro:.4f} | F1 micro: {f1_micro:.4f}\")\n",
    "\n",
    "        # --- NEW: Append metrics for plotting ---\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        val_f1_macros.append(f1_macro)\n",
    "\n",
    "        if f1_macro > best_val_f1:\n",
    "            best_val_f1 = f1_macro\n",
    "            torch.save({\"model_state\": model.state_dict(), \"class_names\": class_names, \"epoch\": epoch}, MODEL_SAVE_PATH)\n",
    "            print(f\"Saved best model to {MODEL_SAVE_PATH} (F1 macro: {best_val_f1:.4f})\")\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    # --- NEW: Plot training metrics after loop ---\n",
    "    print(\"\\nTraining complete. Plotting metrics...\")\n",
    "    plot_metrics(train_losses, val_losses, val_f1_macros)\n",
    "\n",
    "\n",
    "    # Test evaluation\n",
    "    print(f\"\\nLoading best model from {MODEL_SAVE_PATH} for test evaluation...\")\n",
    "    if not os.path.exists(MODEL_SAVE_PATH):\n",
    "        print(\"Error: Best model was not saved. Skipping test evaluation.\")\n",
    "        return\n",
    "        \n",
    "    ckpt = torch.load(MODEL_SAVE_PATH, map_location=DEVICE)\n",
    "    model.load_state_dict(ckpt[\"model_state\"])\n",
    "    \n",
    "    # --- UPDATED: Pass criterion to validate (though test loss isn't used here, it's good practice) ---\n",
    "    _, test_labels, test_logits, test_fns = validate(model, test_loader, criterion, DEVICE)\n",
    "    test_preds = np.argmax(test_logits, axis=1)\n",
    "    f1_macro = f1_score(test_labels, test_preds, average=\"macro\", zero_division=0)\n",
    "    f1_micro = f1_score(test_labels, test_preds, average=\"micro\", zero_division=0)\n",
    "    print(f\"Test F1 macro: {f1_macro:.4f} | F1 micro: {f1_micro:.4f}\")\n",
    "    \n",
    "    # --- NEW: Classification Report and Confusion Matrix ---\n",
    "    print(\"\\nGenerating Classification Report...\")\n",
    "    report = classification_report(test_labels, test_preds, target_names=class_names, zero_division=0)\n",
    "    print(report)\n",
    "    with open(\"classification_report.txt\", \"w\") as f:\n",
    "        f.write(report)\n",
    "    print(\"Saved classification_report.txt\")\n",
    "\n",
    "    print(\"\\nGenerating Confusion Matrix...\")\n",
    "    plot_confusion_matrix(test_labels, test_preds, class_names, save_path=\"test_confusion_matrix.png\")\n",
    "\n",
    "\n",
    "    # Save test predictions\n",
    "    test_probs = torch.softmax(torch.tensor(test_logits), dim=1).numpy()\n",
    "    out_df = pd.DataFrame(test_probs, columns=class_names)\n",
    "    out_df[\"filename\"] = test_fns\n",
    "    out_df = out_df[[\"filename\"] + class_names]\n",
    "    out_df.to_csv(\"test_predictions_multiclass.csv\", index=False)\n",
    "    print(\"Saved test_predictions_multiclass.csv\")\n",
    "\n",
    "    # -------------------------\n",
    "    # Example Grad-CAM explanation (replaces previous LIME example)\n",
    "    # -------------------------\n",
    "    print(\"\\nGenerating Grad-CAM example...\")\n",
    "    # We'll produce one Grad-CAM image for a test example matching the first class in class_names (if present).\n",
    "    target_class_name = class_names[0]\n",
    "    target_idx = 0\n",
    "\n",
    "    # Create GradCAM with hook on cnn_features\n",
    "    gradcam = GradCAM(model, target_module_name=\"cnn_features\", device=DEVICE)\n",
    "\n",
    "    # Find a test example of the chosen class (based on ground truth)\n",
    "    found_example = False\n",
    "    if len(test_ds) == 0:\n",
    "        print(\"Test dataset is empty, cannot generate Grad-CAM.\")\n",
    "    else:\n",
    "        for i in range(len(test_ds)):\n",
    "            img_tensor, label_idx, fn = test_ds[i]\n",
    "            if label_idx == target_idx:\n",
    "                # Find the correct full path to the original image\n",
    "                img_path = os.path.join(test_dir, fn)\n",
    "                if not os.path.exists(img_path):\n",
    "                    img_path = os.path.join(test_dir, \"images\", fn)\n",
    "                \n",
    "                if not os.path.exists(img_path):\n",
    "                    print(f\"Warning: Could not find original image {fn} for Grad-CAM. Skipping.\")\n",
    "                    continue\n",
    "\n",
    "                # prepare input batch\n",
    "                input_tensor = img_tensor.unsqueeze(0).to(DEVICE)  # [1,3,H,W]\n",
    "                # generate cam for the true class\n",
    "                cam = gradcam.generate_cam(input_tensor, target_index=target_idx)  # HxW numpy\n",
    "                # read original image for overlay (un-normalize)\n",
    "                orig_pil = Image.open(img_path).convert(\"RGB\").resize((IMAGE_SIZE, IMAGE_SIZE))\n",
    "                orig = np.array(orig_pil).astype(np.uint8)\n",
    "\n",
    "                # create heatmap overlay\n",
    "                cmap = plt.get_cmap(\"jet\")\n",
    "                heatmap = cmap(cam)[:, :, :3]  # HxWx3\n",
    "                heatmap = (heatmap * 255).astype(np.uint8)\n",
    "\n",
    "                overlay = (0.6 * orig.astype(float) + 0.4 * heatmap.astype(float)).astype(np.uint8)\n",
    "                \n",
    "                # --- NEW: Side-by-side plotting ---\n",
    "                out_img = f\"gradcam_{os.path.basename(fn)}_{target_class_name.strip()}.png\"\n",
    "                \n",
    "                fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "                \n",
    "                axes[0].imshow(orig)\n",
    "                axes[0].set_title(\"Original Image\")\n",
    "                axes[0].axis('off')\n",
    "                \n",
    "                axes[1].imshow(cam, cmap='jet')\n",
    "                axes[1].set_title(\"Grad-CAM Heatmap\")\n",
    "                axes[1].axis('off')\n",
    "\n",
    "                axes[2].imshow(overlay)\n",
    "                axes[2].set_title(\"Overlay\")\n",
    "                axes[2].axis('off')\n",
    "\n",
    "                fig.suptitle(f\"Grad-CAM for: {target_class_name.strip()} (File: {fn})\", fontsize=16)\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(out_img, bbox_inches='tight')\n",
    "                plt.close(fig)\n",
    "                \n",
    "                print(\"Saved Grad-CAM side-by-side plot:\", out_img)\n",
    "                found_example = True\n",
    "                break\n",
    "        \n",
    "        if not found_example:\n",
    "            print(f\"Could not find a test example for class '{target_class_name}' to generate Grad-CAM.\")\n",
    "\n",
    "    # remove hooks\n",
    "    gradcam.remove_hooks()\n",
    "    print(\"Pipeline complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
