{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This pipeline combines DenseNet-121 as the CNN backbone, Swin Transformer-Tiny as the Vision Transformer branch, CBAM for the multi-scale attention module, and LIME for post-hoc explainability, creating a novel and interpretable framework for fetal brain abnormality detection from ultrasound images.\n",
        "\n",
        "**Pipeline Summary**\n",
        "\n",
        "**Data Acquisition and Preprocessing**\n",
        "\n",
        "Uses a fetal brain ultrasound dataset (1,768 images, e.g., Roboflow) across normal and abnormal classes. Preprocessing includes resizing (for CNN/ViT compatibility), normalization, augmentations (to address small data size), and appropriate train-validation-test splits.\n",
        "\n",
        "**CNN Backbone: DenseNet-121**\n",
        "\n",
        "DenseNet-121 extracts local spatial features such as subtle textural changes, anatomical boundaries, and brain structural outlines from ultrasound scans, enhancing detection of localized abnormalities.\n",
        "\n",
        "**Vision Transformer Branch: Swin Transformer-Tiny**\n",
        "\n",
        "Swin Transformer-Tiny provides global contextual understanding, capturing long-range dependencies (such as overall skull shape, large lesions, or abnormal cavity enlargement) that complement the CNN’s localized focus.\n",
        "\n",
        "**Multi-Scale Attention with CBAM**\n",
        "\n",
        "The Convolutional Block Attention Module (CBAM) refines features at both channel and spatial levels, emphasizing image regions and feature groups critical for distinguishing similar abnormalities. This ensures robust attention to both coarse and fine anatomical structures across multiple resolutions.\n",
        "\n",
        "**Feature Fusion**\n",
        "\n",
        "Features from CNN (DenseNet-121) and ViT (Swin Transformer-Tiny) branches are fused (often via concatenation and another multi-scale attention layer) and passed through dense layers to produce unified representations for classification.\n",
        "\n",
        "**Classification**\n",
        "\n",
        "Unified features are processed via fully connected, dropout, and softmax layers for multi-class prediction: normal or subtypes such as ventriculomegaly, encephalocele, holoprosencephaly, and hemorrhage.\n",
        "\n",
        "**Explainability with LIME**\n",
        "\n",
        "LIME (Local Interpretable Model-agnostic Explanations) is used post-classification to generate pixel-level, visually interpretable heatmaps, pinpointing which image areas influenced each decision. This satisfies the need for model transparency, regulatory compliance, and clinician trust—key requirements in medical AI applications.\n",
        "\n",
        "**Key Novelty:**\n",
        "\n",
        "Hybrid CNN-ViT architecture enables robust detection across local and global feature scales.\n",
        "\n",
        "Multi-scale attention (CBAM) ensures adaptive focus on medically relevant regions.\n",
        "\n",
        "LIME provides granular, model-agnostic explainability, addressing the research gap in transparent, clinically deployable fetal abnormality detection."
      ],
      "metadata": {
        "id": "OIHCrxcsQGnu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgPtCL68P6uS",
        "outputId": "b20a7e65-1b34-44f6-dab2-6e1b1947be9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount data from drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision timm scikit-learn pandas pillow tqdm opencv-python matplotlib seaborn lime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHPAnEJbX_-E",
        "outputId": "66346151-0b7f-48f6-bff8-336113c0ab35"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.12/dist-packages (1.0.19)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Collecting lime\n",
            "  Downloading lime-0.2.0.1.tar.gz (275 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.7/275.7 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from timm) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (from timm) (0.35.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from timm) (0.6.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.4)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.12/dist-packages (from lime) (0.25.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.12->lime) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.12->lime) (2025.9.9)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.12->lime) (0.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (1.1.10)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub->timm) (2025.8.3)\n",
            "Building wheels for collected packages: lime\n",
            "  Building wheel for lime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lime: filename=lime-0.2.0.1-py3-none-any.whl size=283834 sha256=5a93a901b3c1cedb783e4d748aed9e928a65ea3341ef74493f46003372dbccaf\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/5d/0e/4b4fff9a47468fed5633211fb3b76d1db43fe806a17fb7486a\n",
            "Successfully built lime\n",
            "Installing collected packages: lime\n",
            "Successfully installed lime-0.2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "fetal_brain_pipeline_multiclass.py\n",
        "\n",
        "End-to-end pipeline (multi-class):\n",
        "- dataset loader (reads <split>/_classes.csv with one-hot labels)\n",
        "- model: DenseNet-121 (CNN) + Swin-Tiny (ViT) + CBAM (attention) + fusion head\n",
        "- training loop (CrossEntropyLoss)\n",
        "- validation/test metrics (per-class AUC, F1 macro/micro)\n",
        "- LIME explanation generation (save heatmaps)\n",
        "\n",
        "Author: ChatGPT\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as T\n",
        "import timm\n",
        "\n",
        "from sklearn.metrics import roc_auc_score, f1_score, precision_recall_fscore_support\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "# LIME imports\n",
        "from lime import lime_image\n",
        "from skimage.segmentation import mark_boundaries\n",
        "\n",
        "# -------------------------\n",
        "# Config / hyperparameters\n",
        "# -------------------------\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "DATA_ROOT = \"/content/drive/MyDrive/Amrita/Sem5/ML/Classification_Dataset/\"\n",
        "BATCH_SIZE = 16\n",
        "IMAGE_SIZE = 224\n",
        "NUM_EPOCHS = 10\n",
        "LR = 1e-4\n",
        "WEIGHT_DECAY = 1e-4\n",
        "NUM_WORKERS = 6\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "MODEL_SAVE_PATH = \"best_model_multiclass.pth\"\n",
        "NUM_CLASSES = 11  # update if different; matches your CSV header count\n",
        "\n",
        "# -------------------------\n",
        "# Helper: read CSV, get classes\n",
        "# -------------------------\n",
        "def read_classes_from_csv(csv_path: str):\n",
        "    df = pd.read_csv(csv_path, sep=None, engine=\"python\")\n",
        "    class_cols = list(df.columns[1:])\n",
        "    return class_cols\n",
        "\n",
        "# -------------------------\n",
        "# Dataset\n",
        "# -------------------------\n",
        "class FetalUSDataset(Dataset):\n",
        "    def __init__(self, split_dir: str, image_size=224, transform=None):\n",
        "        csv_path = os.path.join(split_dir, \"_classes.csv\")\n",
        "        assert os.path.exists(csv_path), f\"CSV not found: {csv_path}\"\n",
        "        self.df = pd.read_csv(csv_path, sep=None, engine=\"python\")\n",
        "        self.dir = split_dir\n",
        "        self.filenames = self.df.iloc[:, 0].astype(str).values\n",
        "        self.labels = self.df.iloc[:, 1:].astype(int).values\n",
        "        self.transform = transform if transform else self.default_transform(image_size)\n",
        "        self.image_size = image_size\n",
        "\n",
        "    def default_transform(self, image_size):\n",
        "        return T.Compose([\n",
        "            T.Resize((image_size, image_size)),\n",
        "            T.Grayscale(num_output_channels=3),\n",
        "            T.ToTensor(),\n",
        "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.filenames)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        fn = self.filenames[idx]\n",
        "        img_path = os.path.join(self.dir, fn)\n",
        "        if not os.path.exists(img_path):\n",
        "            alt = os.path.join(self.dir, \"images\", fn)\n",
        "            if os.path.exists(alt):\n",
        "                img_path = alt\n",
        "            else:\n",
        "                raise FileNotFoundError(f\"{img_path} missing\")\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        x = self.transform(img)\n",
        "        y = torch.tensor(np.argmax(self.labels[idx]), dtype=torch.long)  # convert one-hot → class index\n",
        "        return x, y, fn\n",
        "\n",
        "# -------------------------\n",
        "# CBAM implementation\n",
        "# -------------------------\n",
        "class BasicConv(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes, kernel_size, stride=1, padding=0, bias=False):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size, stride=stride, padding=padding, bias=bias)\n",
        "        self.bn = nn.BatchNorm2d(out_planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "    def forward(self, x):\n",
        "        return self.relu(self.bn(self.conv(x)))\n",
        "\n",
        "class ChannelAttention(nn.Module):\n",
        "    def __init__(self, in_planes, ratio=16):\n",
        "        super().__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
        "        self.fc1 = nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc2 = nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    def forward(self, x):\n",
        "        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))\n",
        "        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))\n",
        "        return self.sigmoid(avg_out + max_out)\n",
        "\n",
        "class SpatialAttention(nn.Module):\n",
        "    def __init__(self, kernel_size=7):\n",
        "        super().__init__()\n",
        "        padding = 3 if kernel_size==7 else 1\n",
        "        self.conv = BasicConv(2, 1, kernel_size, padding=padding, bias=False)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    def forward(self, x):\n",
        "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
        "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
        "        x_cat = torch.cat([avg_out, max_out], dim=1)\n",
        "        return self.sigmoid(self.conv(x_cat))\n",
        "\n",
        "class CBAM(nn.Module):\n",
        "    def __init__(self, in_planes, ratio=16, kernel_size=7):\n",
        "        super().__init__()\n",
        "        self.channel_att = ChannelAttention(in_planes, ratio)\n",
        "        self.spatial_att = SpatialAttention(kernel_size)\n",
        "    def forward(self, x):\n",
        "        x_out = x * self.channel_att(x)\n",
        "        return x_out * self.spatial_att(x_out)\n",
        "\n",
        "# -------------------------\n",
        "# HybridNet: DenseNet + CBAM + Swin + Fusion\n",
        "# -------------------------\n",
        "class HybridNet(nn.Module):\n",
        "    def __init__(self, densenet_model_name=\"densenet121\", swin_model_name=\"swin_base_patch4_window7_224\", num_classes=11):\n",
        "        super().__init__()\n",
        "\n",
        "        # CNN branch\n",
        "        dnet = timm.create_model(densenet_model_name, pretrained=True)\n",
        "        self.cnn_features = dnet.features\n",
        "        self.cbam = CBAM(dnet.num_features)\n",
        "        self.cnn_gap = nn.AdaptiveAvgPool2d((1,1))\n",
        "        cnn_feat_dim = dnet.num_features\n",
        "\n",
        "        # Swin branch\n",
        "        self.swin = timm.create_model(swin_model_name, pretrained=True, num_classes=0, global_pool=\"avg\")\n",
        "        swin_feat_dim = self.swin.num_features\n",
        "        self.swin_pool = nn.AdaptiveAvgPool2d((1,1))\n",
        "\n",
        "        # Fusion + classifier\n",
        "        fusion_in_features = cnn_feat_dim + swin_feat_dim  # automatically 1024 + 1007\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(fusion_in_features, 512),  # use correct input dim\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # CNN\n",
        "        cnn_feat = self.cnn_features(x)\n",
        "        cnn_feat = self.cbam(cnn_feat)\n",
        "        cnn_vec = torch.flatten(self.cnn_gap(cnn_feat), 1)\n",
        "        # Swin\n",
        "        swin_feat = self.swin.forward_features(x)\n",
        "        swin_vec = torch.flatten(self.swin_pool(swin_feat), 1)\n",
        "        # Fusion\n",
        "        fused = torch.cat([cnn_vec, swin_vec], dim=1)\n",
        "        fused = self.fusion_dropout(fused)\n",
        "        logits = self.classifier(fused)\n",
        "        return logits\n",
        "\n",
        "# -------------------------\n",
        "# Training / validation\n",
        "# -------------------------\n",
        "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for imgs, labels, _ in tqdm(loader, desc=\"Train batches\"):\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(imgs)\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * imgs.size(0)\n",
        "    return running_loss / len(loader.dataset)\n",
        "\n",
        "@torch.no_grad()\n",
        "def validate(model, loader, device):\n",
        "    model.eval()\n",
        "    all_logits, all_labels, filenames = [], [], []\n",
        "    for imgs, labels, fns in tqdm(loader, desc=\"Validation batches\"):\n",
        "        imgs = imgs.to(device)\n",
        "        logits = model(imgs)\n",
        "        all_logits.append(logits.detach().cpu().numpy())\n",
        "        all_labels.append(labels.numpy())\n",
        "        filenames.extend(fns)\n",
        "    all_logits = np.vstack(all_logits)\n",
        "    all_labels = np.hstack(all_labels)\n",
        "    return all_labels, all_logits, filenames\n",
        "\n",
        "# -------------------------\n",
        "# LIME explainer\n",
        "# -------------------------\n",
        "class LimeExplainerWrapper:\n",
        "    def __init__(self, model, class_names, device=\"cpu\", transform=None):\n",
        "        self.model = model\n",
        "        self.model.eval()\n",
        "        self.class_names = class_names\n",
        "        self.device = device\n",
        "        self.transform = transform if transform else T.Compose([\n",
        "            T.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "            T.ToTensor(),\n",
        "            T.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
        "        ])\n",
        "    def predict_proba(self, imgs_rgb):\n",
        "        batch = []\n",
        "        for im in imgs_rgb:\n",
        "            pil = Image.fromarray(im.astype(np.uint8))\n",
        "            x = self.transform(pil)\n",
        "            batch.append(x)\n",
        "        x = torch.stack(batch).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            logits = self.model(x)\n",
        "            probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
        "        return probs\n",
        "    def explain_image(self, img_path, label_idx, top_labels=1, num_samples=1000, hide_color=0):\n",
        "        explainer = lime_image.LimeImageExplainer()\n",
        "        image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
        "        fn = lambda imgs: self.predict_proba(np.array(imgs))\n",
        "        explanation = explainer.explain_instance(image, fn, top_labels=top_labels, hide_color=hide_color, num_samples=num_samples)\n",
        "        temp, mask = explanation.get_image_and_mask(label_idx, positive_only=True, num_features=5, hide_rest=False)\n",
        "        vis = mark_boundaries(temp/255.0, mask)\n",
        "        return vis, explanation\n",
        "\n",
        "# -------------------------\n",
        "# Main\n",
        "# -------------------------\n",
        "def main():\n",
        "    # splits\n",
        "    train_dir, valid_dir, test_dir = [os.path.join(DATA_ROOT, s) for s in [\"train\",\"valid\",\"test\"]]\n",
        "    class_names = read_classes_from_csv(os.path.join(train_dir,\"_classes.csv\"))\n",
        "    global NUM_CLASSES\n",
        "    NUM_CLASSES = len(class_names)\n",
        "    print(\"Classes:\", class_names)\n",
        "\n",
        "    # transforms\n",
        "    train_transform = T.Compose([\n",
        "        T.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "        T.RandomHorizontalFlip(),\n",
        "        T.RandomRotation(10),\n",
        "        T.ColorJitter(0.1,0.1),\n",
        "        T.Grayscale(3),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "    ])\n",
        "    val_transform = T.Compose([\n",
        "        T.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "        T.Grayscale(3),\n",
        "        T.ToTensor(),\n",
        "        T.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "    ])\n",
        "\n",
        "    train_ds = FetalUSDataset(train_dir, IMAGE_SIZE, train_transform)\n",
        "    val_ds = FetalUSDataset(valid_dir, IMAGE_SIZE, val_transform)\n",
        "    test_ds = FetalUSDataset(test_dir, IMAGE_SIZE, val_transform)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
        "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
        "    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
        "\n",
        "    # model, criterion, optimizer\n",
        "    model = HybridNet(num_classes=NUM_CLASSES).to(DEVICE)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)\n",
        "\n",
        "    best_val_f1 = -1.0\n",
        "    for epoch in range(1, NUM_EPOCHS+1):\n",
        "        print(f\"\\nEpoch {epoch}/{NUM_EPOCHS}\")\n",
        "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, DEVICE)\n",
        "        print(f\"Train loss: {train_loss:.4f}\")\n",
        "\n",
        "        val_labels, val_logits, _ = validate(model, val_loader, DEVICE)\n",
        "        val_preds = np.argmax(val_logits, axis=1)\n",
        "        f1_macro = f1_score(val_labels, val_preds, average=\"macro\")\n",
        "        f1_micro = f1_score(val_labels, val_preds, average=\"micro\")\n",
        "        print(f\"Val F1 macro: {f1_macro:.4f} | F1 micro: {f1_micro:.4f}\")\n",
        "\n",
        "        if f1_macro > best_val_f1:\n",
        "            best_val_f1 = f1_macro\n",
        "            torch.save({\"model_state\": model.state_dict(), \"class_names\": class_names, \"epoch\": epoch}, MODEL_SAVE_PATH)\n",
        "            print(\"Saved best model.\")\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    # Test evaluation\n",
        "    ckpt = torch.load(MODEL_SAVE_PATH, map_location=DEVICE)\n",
        "    model.load_state_dict(ckpt[\"model_state\"])\n",
        "    test_labels, test_logits, test_fns = validate(model, test_loader, DEVICE)\n",
        "    test_preds = np.argmax(test_logits, axis=1)\n",
        "    f1_macro = f1_score(test_labels, test_preds, average=\"macro\")\n",
        "    f1_micro = f1_score(test_labels, test_preds, average=\"micro\")\n",
        "    print(\"Test F1 macro:\", f1_macro, \"F1 micro:\", f1_micro)\n",
        "\n",
        "    # Save test predictions\n",
        "    test_probs = torch.softmax(torch.tensor(test_logits), dim=1).numpy()\n",
        "    out_df = pd.DataFrame(test_probs, columns=class_names)\n",
        "    out_df[\"filename\"] = test_fns\n",
        "    out_df = out_df[[\"filename\"] + class_names]\n",
        "    out_df.to_csv(\"test_predictions_multiclass.csv\", index=False)\n",
        "    print(\"Saved test_predictions_multiclass.csv\")\n",
        "\n",
        "    # Example LIME explanation\n",
        "    explainer = LimeExplainerWrapper(model, class_names, DEVICE, val_transform)\n",
        "    target_class_name = class_names[0]  # e.g., first class\n",
        "    idx = class_names.index(target_class_name)\n",
        "    for i in range(len(test_ds)):\n",
        "        _, label_idx, fn = test_ds[i]\n",
        "        if label_idx == idx:\n",
        "            img_path = os.path.join(test_dir, fn)\n",
        "            vis, _ = explainer.explain_image(img_path, label_idx=idx, num_samples=800)\n",
        "            out_img = f\"lime_{os.path.basename(fn)}_{target_class_name}.png\"\n",
        "            plt.imsave(out_img, vis)\n",
        "            print(\"Saved LIME visualization:\", out_img)\n",
        "            break\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        },
        "id": "R5J74kQMXjgI",
        "outputId": "f3d29576-d02e-4feb-abb6-a9b2f42f6243"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes: [' anold-chiari-malformation', ' arachnoid-cyst', ' cerebellah-hypoplasia', ' colphocephaly', ' encephalocele', ' holoprosencephaly', ' hydracenphaly', ' intracranial-hemorrdge', ' intracranial-tumor', ' m-magna', ' mild-ventriculomegaly', ' moderate-ventriculomegaly', ' normal', ' polencephaly', ' severe-ventriculomegaly', ' vein-of-galen']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train batches:   0%|          | 0/89 [00:47<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'HybridNet' object has no attribute 'fusion_dropout'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1868043020.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1868043020.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    298\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nEpoch {epoch}/{NUM_EPOCHS}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Train loss: {train_loss:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1868043020.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, loader, optimizer, criterion, device)\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1868043020.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0;31m# Fusion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mfused\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcnn_vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswin_vec\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mfused\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfusion_dropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfused\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfused\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1960\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1961\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1962\u001b[0;31m         raise AttributeError(\n\u001b[0m\u001b[1;32m   1963\u001b[0m             \u001b[0;34mf\"'{type(self).__name__}' object has no attribute '{name}'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1964\u001b[0m         )\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'HybridNet' object has no attribute 'fusion_dropout'"
          ]
        }
      ]
    }
  ]
}